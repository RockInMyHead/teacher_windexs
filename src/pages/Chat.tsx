import React, { useState, useRef, useEffect } from 'react';
import { Button } from '@/components/ui/button';
import { Input } from '@/components/ui/input';
import { Card, CardContent, CardHeader, CardTitle } from '@/components/ui/card';
import { ScrollArea } from '@/components/ui/scroll-area';
import { Avatar, AvatarFallback } from '@/components/ui/avatar';
import { Brain, Send, User, ArrowLeft, MessageCircle, Upload, FileText, Image, File, X, Camera, Volume2, VolumeX, Mic, MicOff, Trash2 } from 'lucide-react';
import { useNavigate } from 'react-router-dom';
import { useAuth } from '@/contexts/AuthContext';
import { OpenAITTS, isTTSAvailable } from '@/lib/openaiTTS';

// Global types for Speech Recognition API
interface SpeechRecognitionEvent extends Event {
  resultIndex: number;
  results: SpeechRecognitionResultList;
}

interface SpeechRecognitionResultList {
  [index: number]: SpeechRecognitionResult;
  length: number;
}

interface SpeechRecognitionResult {
  [index: number]: SpeechRecognitionAlternative;
  length: number;
  isFinal: boolean;
}

interface SpeechRecognitionAlternative {
  transcript: string;
  confidence: number;
}

interface SpeechRecognition extends EventTarget {
  continuous: boolean;
  interimResults: boolean;
  lang: string;
  start(): void;
  stop(): void;
  onstart: ((event: Event) => void) | null;
  onresult: ((event: SpeechRecognitionEvent) => void) | null;
  onerror: ((event: any) => void) | null;
  onend: ((event: Event) => void) | null;
}

declare var SpeechRecognition: {
  prototype: SpeechRecognition;
  new(): SpeechRecognition;
};

declare global {
  interface Window {
    SpeechRecognition: typeof SpeechRecognition;
    webkitSpeechRecognition: typeof SpeechRecognition;
  }
}

interface Message {
  id: string;
  role: 'user' | 'assistant';
  content: string;
  timestamp: Date;
  ttsPlayed?: boolean;
}


const Chat = () => {
  const { isAuthenticated } = useAuth();
  const navigate = useNavigate();
  const [messages, setMessages] = useState<Message[]>([]);
  const [inputMessage, setInputMessage] = useState('');
  const [isLoading, setIsLoading] = useState(false);
  const [uploadedFiles, setUploadedFiles] = useState<File[]>([]);
  const [isProcessingFile, setIsProcessingFile] = useState(false);
  const [isCameraActive, setIsCameraActive] = useState(false);
  const [cameraStream, setCameraStream] = useState<MediaStream | null>(null);
  const [capturedImage, setCapturedImage] = useState<string | null>(null);
  const [speakingMessageId, setSpeakingMessageId] = useState<string | null>(null);
  const [isTtsEnabled, setIsTtsEnabled] = useState(false);
  const [isVoiceChatActive, setIsVoiceChatActive] = useState(false);
  const [isListening, setIsListening] = useState(false);
  const [apiKeyStatus, setApiKeyStatus] = useState<'checking' | 'valid' | 'invalid' | 'error'>('checking');
  const [ttsInterrupted, setTtsInterrupted] = useState(false);
  const [currentSentence, setCurrentSentence] = useState<number>(0);
  const [totalSentences, setTotalSentences] = useState<number>(0);
  const [isGeneratingTTS, setIsGeneratingTTS] = useState(false);
  const [showSphere, setShowSphere] = useState(false);
  const scrollAreaRef = useRef<HTMLDivElement>(null);
  const inputRef = useRef<HTMLInputElement>(null);
  const fileInputRef = useRef<HTMLInputElement>(null);
  const videoRef = useRef<HTMLVideoElement>(null);
  const canvasRef = useRef<HTMLCanvasElement>(null);
  const recognitionRef = useRef<SpeechRecognition | null>(null);
  const ttsContinueRef = useRef<boolean>(true);
  const audioContextRef = useRef<AudioContext | null>(null);
  const currentAudioRef = useRef<HTMLAudioElement | null>(null);
  const soundIntervalRef = useRef<NodeJS.Timeout | null>(null);
  const interruptionCheckIntervalsRef = useRef<Set<NodeJS.Timeout>>(new Set());

  // Audio feedback functions
  const initAudioContext = () => {
    if (!audioContextRef.current) {
      audioContextRef.current = new (window.AudioContext || (window as any).webkitAudioContext)();
    }
    return audioContextRef.current;
  };

  const playBeep = async (frequency: number = 800, duration: number = 200, type: OscillatorType = 'sine') => {
    try {
      const audioContext = initAudioContext();
      if (audioContext.state === 'suspended') {
        await audioContext.resume();
      }

      const oscillator = audioContext.createOscillator();
      const gainNode = audioContext.createGain();

      oscillator.connect(gainNode);
      gainNode.connect(audioContext.destination);

      oscillator.frequency.value = frequency;
      oscillator.type = type;

      gainNode.gain.setValueAtTime(0.1, audioContext.currentTime);
      gainNode.gain.exponentialRampToValueAtTime(0.01, audioContext.currentTime + duration / 1000);

      oscillator.start(audioContext.currentTime);
      oscillator.stop(audioContext.currentTime + duration / 1000);
    } catch (error) {
      console.warn('Could not play audio feedback:', error);
    }
  };

  const startContinuousSound = (frequency: number = 600, interval: number = 800) => {
    if (soundIntervalRef.current) {
      clearInterval(soundIntervalRef.current);
    }

    soundIntervalRef.current = setInterval(() => {
      playBeep(frequency, 100, 'sine');
    }, interval);
  };


  const stopContinuousSound = () => {
    if (soundIntervalRef.current) {
      clearInterval(soundIntervalRef.current);
      soundIntervalRef.current = null;
    }
  };

  // Clear all interruption check intervals
  const clearAllInterruptionChecks = () => {
    interruptionCheckIntervalsRef.current.forEach(interval => {
      clearInterval(interval);
    });
    interruptionCheckIntervalsRef.current.clear();
  };


  // Auto TTS for new messages when enabled
  useEffect(() => {
    if (isTtsEnabled && messages.length > 0) {
      const lastMessage = messages[messages.length - 1];
      // Only auto-speak assistant messages, not user messages, and only if not already speaking
      if (lastMessage.role === 'assistant' && !lastMessage.ttsPlayed && !OpenAITTS.isPlaying()) {
        // Mark as played to avoid re-playing
        lastMessage.ttsPlayed = true;
        speakTextBySentences(lastMessage.content, lastMessage.id); // Use sentence-by-sentence speaking
      }
    }
  }, [messages, isTtsEnabled]);

  // Initialize Speech Recognition
  useEffect(() => {
    console.log('üé§ Initializing Speech Recognition...');

    // Try different ways to access Speech Recognition API
    let SpeechRecognition = null;

    if ('SpeechRecognition' in window) {
      SpeechRecognition = window.SpeechRecognition;
      console.log('‚úÖ Found SpeechRecognition');
    } else if ('webkitSpeechRecognition' in window) {
      SpeechRecognition = window.webkitSpeechRecognition;
      console.log('‚úÖ Found webkitSpeechRecognition');
    } else if ('mozSpeechRecognition' in window) {
      SpeechRecognition = window.mozSpeechRecognition;
      console.log('‚úÖ Found mozSpeechRecognition');
    } else if ('msSpeechRecognition' in window) {
      SpeechRecognition = window.msSpeechRecognition;
      console.log('‚úÖ Found msSpeechRecognition');
    }

    if (SpeechRecognition) {
      try {
        recognitionRef.current = new SpeechRecognition();
        recognitionRef.current.continuous = false;
        recognitionRef.current.interimResults = false;
        recognitionRef.current.lang = 'ru-RU';
        console.log('‚úÖ Speech Recognition initialized successfully');
      } catch (error) {
        console.error('‚ùå Error initializing Speech Recognition:', error);
      }
    } else {
      console.log('‚ùå No Speech Recognition API found');
    }
  }, []);

  // Cleanup camera, TTS and speech recognition on unmount
  useEffect(() => {
    return () => {
      if (cameraStream) {
        cameraStream.getTracks().forEach(track => track.stop());
      }
      // Stop TTS
      OpenAITTS.stop();
      // Stop speech recognition
      if (recognitionRef.current) {
        recognitionRef.current.stop();
      }
    };
  }, [cameraStream]);


  // Function to split text into sentences
  const splitIntoSentences = (text: string): string[] => {
    // Split by sentence endings, but keep the punctuation
    const sentences = text.split(/(?<=[.!?])\s+/);
    return sentences.filter(sentence => sentence.trim().length > 0);
  };

  // Function to speak text sentence by sentence for faster TTS
  const speakTextBySentences = async (text: string, messageId: string) => {
    if (!isTTSAvailable()) {
      alert('OpenAI API –∫–ª—é—á –Ω–µ –Ω–∞—Å—Ç—Ä–æ–µ–Ω. –ü—Ä–æ–≤–µ—Ä—å—Ç–µ –ø–µ—Ä–µ–º–µ–Ω–Ω—ã–µ –æ–∫—Ä—É–∂–µ–Ω–∏—è.');
      return;
    }

    ttsContinueRef.current = true; // Reset continuation flag

    try {
      // Stop any currently speaking
      OpenAITTS.stop();
      if (currentAudioRef.current) {
        currentAudioRef.current.pause();
        currentAudioRef.current.currentTime = 0;
        currentAudioRef.current = null;
      }

      setSpeakingMessageId(messageId);
      setIsGeneratingTTS(true);

      // Split text into sentences
      const sentences = splitIntoSentences(text).filter(s => s.trim().length > 0);
      console.log('üìù Split into sentences:', sentences);

      if (sentences.length === 0) {
        setSpeakingMessageId(null);
        setIsGeneratingTTS(false);
        return;
      }

      // Set total sentences for UI
      setTotalSentences(sentences.length);
      setCurrentSentence(0);

      console.log('üöÄ Starting parallel TTS generation for', sentences.length, 'sentences');

      // Start continuous TTS generation sound (same pattern as LLM)
      startContinuousSound(500, 1800);

      // Generate all audio buffers in parallel
      const generationPromises = sentences.map(async (sentence, index) => {
        try {
          console.log(`üì§ Generating TTS for sentence ${index + 1}:`, sentence.substring(0, 50) + '...');
          const audioBuffer = await OpenAITTS.generateSpeech(sentence, {
            voice: 'alloy',
            speed: 0.9,
            model: 'tts-1'
          });
          console.log(`‚úÖ TTS generated for sentence ${index + 1}`);
          return { audioBuffer, sentence, index };
        } catch (error) {
          console.error(`‚ùå Failed to generate TTS for sentence ${index + 1}:`, error);
          return { audioBuffer: null, sentence, index, error };
        }
      });

      // Wait for all generations to complete
      const audioResults = await Promise.allSettled(generationPromises);
      const successfulResults = audioResults
        .filter((result): result is PromiseFulfilledResult<{ audioBuffer: ArrayBuffer | null; sentence: string; index: number; error?: any }> =>
          result.status === 'fulfilled' && result.value.audioBuffer !== null
        )
        .map(result => result.value);

      console.log('üéâ All TTS generation completed');

      // Stop continuous TTS generation sound
      stopContinuousSound();

      // Helper function to play a sentence
      const playSentence = async (audioBuffer: ArrayBuffer, sentence: string, sentenceNumber: number, totalSentences: number): Promise<void> => {
        return new Promise<void>((resolve, reject) => {
          // Check if TTS was interrupted before starting playback
          if (!ttsContinueRef.current) {
            console.log('üõë TTS interrupted before playback');
            reject(new Error('TTS interrupted'));
            return;
          }

          setCurrentSentence(sentenceNumber);
          console.log(`üîä Playing sentence ${sentenceNumber}/${totalSentences}:`, sentence.substring(0, 50) + '...');

          // Silent playback

          try {
            const blob = new Blob([audioBuffer], { type: 'audio/mpeg' });
            const audioUrl = URL.createObjectURL(blob);
            const audio = new Audio(audioUrl);

            const checkInterruption = () => {
              if (!ttsContinueRef.current) {
                console.log('üõë TTS interrupted during playback');
                audio.pause();
                URL.revokeObjectURL(audioUrl);
                currentAudioRef.current = null;
                reject(new Error('TTS interrupted'));
              }
            };

            // Check for interruption every 100ms
            const interruptionCheck = setInterval(checkInterruption, 100);
            // Register the interval for cleanup
            interruptionCheckIntervalsRef.current.add(interruptionCheck);

            audio.onended = () => {
              clearInterval(interruptionCheck);
              interruptionCheckIntervalsRef.current.delete(interruptionCheck);
              URL.revokeObjectURL(audioUrl);
              currentAudioRef.current = null;
              resolve();
            };

            audio.onerror = (error) => {
              clearInterval(interruptionCheck);
              interruptionCheckIntervalsRef.current.delete(interruptionCheck);
              URL.revokeObjectURL(audioUrl);
              currentAudioRef.current = null;
              reject(error);
            };

            // Store reference to current audio for interruption
            currentAudioRef.current = audio;

            audio.play().catch((error) => {
              clearInterval(interruptionCheck);
              interruptionCheckIntervalsRef.current.delete(interruptionCheck);
              URL.revokeObjectURL(audioUrl);
              currentAudioRef.current = null;
              reject(error);
            });
          } catch (playError) {
            console.error(`Error setting up audio for sentence ${sentenceNumber}:`, playError);
            reject(playError);
          }
        });
      };

      // Play sentences sequentially
      for (let i = 0; i < successfulResults.length; i++) {
        const result = successfulResults[i];

        // Check if TTS was interrupted before starting next sentence
        if (!ttsContinueRef.current) {
          console.log('üõë TTS interrupted before playing sentence', i + 1);
          break;
        }

        try {
          await playSentence(result.audioBuffer!, result.sentence, i + 1, successfulResults.length);

          // Small pause between sentences (only if not interrupted)
          if (i < successfulResults.length - 1 && ttsContinueRef.current) {
            await new Promise<void>((resolve, reject) => {
              const pauseCheck = setInterval(() => {
                if (!ttsContinueRef.current) {
                  clearInterval(pauseCheck);
                  interruptionCheckIntervalsRef.current.delete(pauseCheck);
                  reject(new Error('TTS interrupted during pause'));
                }
              }, 50);

              // Register the pause check interval
              interruptionCheckIntervalsRef.current.add(pauseCheck);

              setTimeout(() => {
                clearInterval(pauseCheck);
                interruptionCheckIntervalsRef.current.delete(pauseCheck);
                if (ttsContinueRef.current) {
                  resolve();
    } else {
                  reject(new Error('TTS interrupted during pause'));
                }
              }, 150);
            });
          }
        } catch (playError) {
          if (playError.message === 'TTS interrupted') {
            console.log('üõë TTS playback interrupted by user');
            break;
          }
          console.error(`Error playing sentence ${i + 1}:`, playError);
          // Continue with next sentence
        }
      }

      // Reset counters and state only if not interrupted
      if (ttsContinueRef.current) {
        setCurrentSentence(0);
        setTotalSentences(0);
        setIsGeneratingTTS(false);
        setSpeakingMessageId(null);
      }

    } catch (error) {
      console.error('Parallel TTS error:', error);
      alert('–û—à–∏–±–∫–∞ –ø—Ä–∏ –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ —Ä–µ—á–∏. –ü–æ–ø—Ä–æ–±—É–π—Ç–µ –µ—â–µ —Ä–∞–∑.');
      setSpeakingMessageId(null);
      setCurrentSentence(0);
      setTotalSentences(0);
      setIsGeneratingTTS(false);
    }
  };

  // Function to speak text using OpenAI TTS (legacy function)
  const speakText = async (text: string, messageId: string, showVisualFeedback: boolean = true) => {
    if (!isTTSAvailable()) {
      alert('OpenAI API –∫–ª—é—á –Ω–µ –Ω–∞—Å—Ç—Ä–æ–µ–Ω. –ü—Ä–æ–≤–µ—Ä—å—Ç–µ –ø–µ—Ä–µ–º–µ–Ω–Ω—ã–µ –æ–∫—Ä—É–∂–µ–Ω–∏—è.');
      return;
    }

    try {
      // Stop any currently speaking
      OpenAITTS.stop();

      if (showVisualFeedback && speakingMessageId === messageId) {
        // If already speaking this message, stop it
        setSpeakingMessageId(null);
        return;
      }

      if (showVisualFeedback) {
        setSpeakingMessageId(messageId);
      }

      // Use sentence-by-sentence speaking for better performance
      await speakTextBySentences(text, messageId);

    } catch (error) {
      console.error('OpenAI TTS error:', error);
      alert('–û—à–∏–±–∫–∞ –ø—Ä–∏ –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ —Ä–µ—á–∏. –ü–æ–ø—Ä–æ–±—É–π—Ç–µ –µ—â–µ —Ä–∞–∑.');
      if (showVisualFeedback) {
        setSpeakingMessageId(null);
      }
    }
  };

  // Function to toggle TTS mode
  const toggleTts = () => {
    if (isTtsEnabled) {
      // Disable TTS
      setIsTtsEnabled(false);
      setSpeakingMessageId(null);
      OpenAITTS.stop();
    } else {
      // Enable TTS
      setIsTtsEnabled(true);
    }
  };

  // Function to start voice chat
  const startVoiceChat = async () => {
    setShowSphere(true);
    console.log('üöÄ Starting voice chat...');
    console.log('üîç isTTSAvailable():', isTTSAvailable());
    console.log('üîç Speech Recognition available:', 'webkitSpeechRecognition' in window && 'SpeechRecognition' in window);
    console.log('üîç OpenAI API key:', import.meta.env.VITE_OPENAI_API_KEY ? 'present' : 'missing');
    console.log('üîç VITE_OPENAI_API_KEY value:', import.meta.env.VITE_OPENAI_API_KEY);

    // No start sound - keeping it silent

    if (!isTTSAvailable()) {
      alert('OpenAI API –∫–ª—é—á –Ω–µ –Ω–∞—Å—Ç—Ä–æ–µ–Ω. –ü—Ä–æ–≤–µ—Ä—å—Ç–µ –ø–µ—Ä–µ–º–µ–Ω–Ω—ã–µ –æ–∫—Ä—É–∂–µ–Ω–∏—è.');
      console.log('‚ùå OpenAI API key not available');
      return;
    }

    console.log('‚úÖ OpenAI API key available');

    console.log('üîç Checking Speech Recognition support...');
    console.log('webkitSpeechRecognition in window:', 'webkitSpeechRecognition' in window);
    console.log('SpeechRecognition in window:', 'SpeechRecognition' in window);
    console.log('navigator.userAgent:', navigator.userAgent);
    console.log('location.protocol:', location.protocol);

    if (!recognitionRef.current) {
      const userAgent = navigator.userAgent;
      const browserName = userAgent.includes('Firefox') ? 'Firefox' :
                         userAgent.includes('Chrome') ? 'Chrome' :
                         userAgent.includes('Safari') && !userAgent.includes('Chrome') ? 'Safari' :
                         userAgent.includes('Edge') ? 'Edge' : '–Ω–µ–∏–∑–≤–µ—Å—Ç–Ω—ã–π –±—Ä–∞—É–∑–µ—Ä';

      const isHTTPS = location.protocol === 'https:';
      const isLocalhost = location.hostname === 'localhost' || location.hostname === '127.0.0.1';

      let reason = '';
      if (!isHTTPS && !isLocalhost) {
        reason = '\n\n–ü—Ä–∏—á–∏–Ω–∞: Web Speech API —Ç—Ä–µ–±—É–µ—Ç HTTPS —Å–æ–µ–¥–∏–Ω–µ–Ω–∏—è.';
      } else if (browserName === 'Safari') {
        reason = '\n\n–ü—Ä–∏–º–µ—á–∞–Ω–∏–µ: Safari –º–æ–∂–µ—Ç —Ç—Ä–µ–±–æ–≤–∞—Ç—å –¥–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω—ã—Ö —Ä–∞–∑—Ä–µ—à–µ–Ω–∏–π –∏–ª–∏ –æ–±–Ω–æ–≤–ª–µ–Ω–∏—è.';
      }

      const choice = confirm(`üé§ –í–∞—à –±—Ä–∞—É–∑–µ—Ä (${browserName}) –Ω–µ –ø–æ–¥–¥–µ—Ä–∂–∏–≤–∞–µ—Ç —Ä–∞—Å–ø–æ–∑–Ω–∞–≤–∞–Ω–∏–µ —Ä–µ—á–∏.${reason}\n\n–î–æ—Å—Ç—É–ø–Ω—ã–µ API:\n‚Ä¢ webkitSpeechRecognition: ${'webkitSpeechRecognition' in window}\n‚Ä¢ SpeechRecognition: ${'SpeechRecognition' in window}\n\n–í—ã–±–µ—Ä–∏—Ç–µ —Ä–µ–∂–∏–º:\n\n‚Ä¢ OK: –¢–µ–∫—Å—Ç–æ–≤—ã–π –≤–≤–æ–¥ —Å –≥–æ–ª–æ—Å–æ–≤—ã–º–∏ –æ—Ç–≤–µ—Ç–∞–º–∏\n‚Ä¢ –û—Ç–º–µ–Ω–∞: –¢–æ–ª—å–∫–æ TTS –¥–ª—è –æ—Ç–≤–µ—Ç–æ–≤ –Ω–∞ —Ç–µ–∫—Å—Ç–æ–≤—ã–µ —Å–æ–æ–±—â–µ–Ω–∏—è`);

      if (choice) {
        // –¢–µ–∫—Å—Ç–æ–≤—ã–π —Ä–µ–∂–∏–º —Å TTS –æ—Ç–≤–µ—Ç–∞–º–∏
        alert('‚úçÔ∏è –¢–µ–∫—Å—Ç–æ–≤—ã–π —Ä–µ–∂–∏–º –∞–∫—Ç–∏–≤–∏—Ä–æ–≤–∞–Ω!\n\n–ü–∏—à–∏—Ç–µ –≤–æ–ø—Ä–æ—Å—ã –≤ —á–∞—Ç–µ - AI –±—É–¥–µ—Ç –æ—Ç–≤–µ—á–∞—Ç—å –≥–æ–ª–æ—Å–æ–º.\n\n–î–ª—è –ø–æ–ª–Ω–æ—Ü–µ–Ω–Ω–æ–≥–æ –≥–æ–ª–æ—Å–æ–≤–æ–≥–æ –æ–±—â–µ–Ω–∏—è —É—Å—Ç–∞–Ω–æ–≤–∏—Ç–µ Chrome –∏–ª–∏ Edge.');
        setIsTtsEnabled(true);
        console.log('üìù Text mode with TTS enabled');
      } else {
        // –¢–æ–ª—å–∫–æ TTS –¥–ª—è –æ—Ç–≤–µ—Ç–æ–≤
        alert('üîä TTS —Ä–µ–∂–∏–º –∞–∫—Ç–∏–≤–∏—Ä–æ–≤–∞–Ω!\n\nAI –±—É–¥–µ—Ç –æ–∑–≤—É—á–∏–≤–∞—Ç—å –æ—Ç–≤–µ—Ç—ã –Ω–∞ –≤–∞—à–∏ —Ç–µ–∫—Å—Ç–æ–≤—ã–µ —Å–æ–æ–±—â–µ–Ω–∏—è.\n\n–î–ª—è –≥–æ–ª–æ—Å–æ–≤–æ–≥–æ –≤–≤–æ–¥–∞ —É—Å—Ç–∞–Ω–æ–≤–∏—Ç–µ Chrome –∏–ª–∏ Edge.');
        setIsTtsEnabled(true);
        console.log('üîä TTS-only mode enabled');
      }
      return;
    }

    console.log('üîç Checking SpeechRecognition availability...');
    if (!recognitionRef.current) {
      console.log('‚ùå SpeechRecognition not initialized');
      alert('–û—à–∏–±–∫–∞ –∏–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏–∏ —Ä–∞—Å–ø–æ–∑–Ω–∞–≤–∞–Ω–∏—è —Ä–µ—á–∏. –ü–æ–ø—Ä–æ–±—É–π—Ç–µ –ø–µ—Ä–µ–∑–∞–≥—Ä—É–∑–∏—Ç—å —Å—Ç—Ä–∞–Ω–∏—Ü—É.');
      return;
    }
    console.log('‚úÖ SpeechRecognition initialized');

    if (isVoiceChatActive) {
      // Stop voice chat
      setIsVoiceChatActive(false);
      setIsListening(false);
      recognitionRef.current.stop();
      OpenAITTS.stop();
      return;
    }

    // Start voice chat
    setIsVoiceChatActive(true);
    setIsListening(true);

    try {
      // Configure for continuous listening with interim results
      recognitionRef.current.continuous = true;
      recognitionRef.current.interimResults = true;

      let isProcessing = false;
      let finalTranscript = '';
      let interimTranscript = '';

      recognitionRef.current.onresult = async (event) => {
        interimTranscript = '';
        finalTranscript = '';

        // Process all results
        for (let i = event.resultIndex; i < event.results.length; i++) {
          const transcript = event.results[i][0].transcript;
          if (event.results[i].isFinal) {
            finalTranscript += transcript;
          } else {
            interimTranscript += transcript;
          }
        }

        console.log('Final:', finalTranscript, 'Interim:', interimTranscript);

        // Interrupt TTS immediately when user starts speaking (even with minimal interim results)
        const shouldInterrupt = (interimTranscript.trim() && interimTranscript.trim().length > 0) ||
                               (finalTranscript.trim() && finalTranscript.trim().length > 0);

        if (shouldInterrupt && (currentAudioRef.current || isGeneratingTTS || speakingMessageId)) {
          console.log('üõë Interrupting TTS - user started speaking');

          // Stop current audio playback
          if (currentAudioRef.current) {
            currentAudioRef.current.pause();
            currentAudioRef.current.currentTime = 0;
            currentAudioRef.current = null;
          }

          // Stop TTS generation and playback
          OpenAITTS.stop();
          ttsContinueRef.current = false;

          // Stop continuous sound if playing
          stopContinuousSound();

          // Clear all interruption check intervals
          clearAllInterruptionChecks();

          // Reset TTS state
          setIsGeneratingTTS(false);
          setSpeakingMessageId(null);
          setCurrentSentence(0);
          setTotalSentences(0);
          setTtsInterrupted(true);

          // Reset interruption flag after delay
          setTimeout(() => setTtsInterrupted(false), 1000);

          console.log('‚úÖ TTS interrupted successfully');
        }

        // If we have a final transcript and not currently processing
        if (finalTranscript.trim() && !isProcessing) {
          isProcessing = true;
          setIsListening(false);

          try {
            // Add user message
            const userMessage: Message = {
              id: Date.now().toString(),
              role: 'user',
              content: finalTranscript.trim(),
              timestamp: new Date(),
            };

            setMessages(prev => [...prev, userMessage]);
            setIsLoading(true);

            // Start continuous thinking sound while AI generates response
            startContinuousSound(500, 1800);

            // Get AI response using GPT-3.5-turbo for faster response with fallback
            let response;
            try {
              response = await fetch(`${window.location.origin}/api/chat/completions`, {
                method: 'POST',
                headers: {
                  'Content-Type': 'application/json',
                },
                body: JSON.stringify({
                  model: 'gpt-3.5-turbo',
                  messages: [
                    {
                      role: 'system',
                      content: `–í—ã - –ø—Ä–æ—Ñ–µ—Å—Å–∏–æ–Ω–∞–ª—å–Ω—ã–π –ø–µ–¥–∞–≥–æ–≥ –∏ —ç–∫—Å–ø–µ—Ä—Ç –≤ –æ–±—Ä–∞–∑–æ–≤–∞–Ω–∏–∏. –í–∞—à–∞ –∑–∞–¥–∞—á–∞ - –æ–±—ä—è—Å–Ω—è—Ç—å –ª—é–±—ã–µ —Ç–µ–º—ã –±—ã—Å—Ç—Ä–æ, –ø–æ–Ω—è—Ç–Ω–æ –∏ –¥–æ—Å—Ç—É–ø–Ω–æ. –î–∞–≤–∞–π—Ç–µ –∫—Ä–∞—Ç–∫–∏–µ, –Ω–æ –ø–æ–ª–Ω—ã–µ –æ–±—ä—è—Å–Ω–µ–Ω–∏—è.

–í–ê–ñ–ù–û: –ü–∏—à–∏—Ç–µ –í–°–ï —á–∏—Å–ª–∞, –¥–∞—Ç—ã, –º–∞—Ç–µ–º–∞—Ç–∏—á–µ—Å–∫–∏–µ –∑–Ω–∞–∫–∏ –∏ –æ–ø–µ—Ä–∞—Ü–∏–∏ –ë–£–ö–í–ê–ú–ò, –∞ –Ω–µ —Ü–∏—Ñ—Ä–∞–º–∏!

–ü—Ä–∞–≤–∏–ª–∞ –Ω–∞–ø–∏—Å–∞–Ω–∏—è:
- –ß–∏—Å–ª–∞: "–¥–≤–∞–¥—Ü–∞—Ç—å —Ç—Ä–∏" –≤–º–µ—Å—Ç–æ "23", "—Å—Ç–æ –ø—è—Ç—å–¥–µ—Å—è—Ç" –≤–º–µ—Å—Ç–æ "150"
- –î–∞—Ç—ã: "–¥–≤–µ–Ω–∞–¥—Ü–∞—Ç–æ–µ –∞–ø—Ä–µ–ª—è —Ç—ã—Å—è—á–∞ –¥–µ–≤—è—Ç—å—Å–æ—Ç —à–µ—Å—Ç—å–¥–µ—Å—è—Ç –ø–µ—Ä–≤–æ–≥–æ –≥–æ–¥–∞" –≤–º–µ—Å—Ç–æ "12 –∞–ø—Ä–µ–ª—è 1961 –≥–æ–¥–∞"
- –ú–∞—Ç–µ–º–∞—Ç–∏–∫–∞: "–¥–≤–∞ –ø–ª—é—Å —Ç—Ä–∏ —Ä–∞–≤–Ω–æ –ø—è—Ç–∏" –≤–º–µ—Å—Ç–æ "2 + 3 = 5"
- –ü—Ä–æ—Ü–µ–Ω—Ç—ã: "—Ç—Ä–∏–¥—Ü–∞—Ç—å –ø—è—Ç—å –ø—Ä–æ—Ü–µ–Ω—Ç–æ–≤" –≤–º–µ—Å—Ç–æ "35%"
- –î—Ä–æ–±–∏: "—Ç—Ä–∏ –ø—è—Ç—ã—Ö" –≤–º–µ—Å—Ç–æ "3/5"
- –°—Ç–µ–ø–µ–Ω–∏: "–¥–≤–∞ –≤ –∫—É–±–µ" –≤–º–µ—Å—Ç–æ "2¬≥"
- –ü—Ä–æ–∏–∑–≤–æ–¥–Ω—ã–µ: "–ø—Ä–æ–∏–∑–≤–æ–¥–Ω–∞—è —Ñ—É–Ω–∫—Ü–∏–∏ —ç—Ñ –æ—Ç –∏–∫—Å" –∏–ª–∏ "–¥–∏—Ñ—Ñ–µ—Ä–µ–Ω—Ü–∏–∞–ª –∏–≥—Ä–µ–∫ –ø–æ –¥–∏—Ñ—Ñ–µ—Ä–µ–Ω—Ü–∏–∞–ª –∏–∫—Å" –≤–º–µ—Å—Ç–æ "f'(x)" –∏–ª–∏ "dy/dx"
- –ò–Ω—Ç–µ–≥—Ä–∞–ª—ã: "–∏–Ω—Ç–µ–≥—Ä–∞–ª –æ—Ç –Ω—É–ª—è –¥–æ –±–µ—Å–∫–æ–Ω–µ—á–Ω–æ—Å—Ç–∏" –≤–º–µ—Å—Ç–æ "‚à´‚àû0"
- –°—É–º–º—ã: "—Å—É–º–º–∞ –æ—Ç –∏ —Ä–∞–≤–Ω—è–µ—Ç—Å—è –µ–¥–∏–Ω–∏—Ü–µ –¥–æ –±–µ—Å–∫–æ–Ω–µ—á–Ω–æ—Å—Ç–∏" –≤–º–µ—Å—Ç–æ "‚àë‚àûi=1"
- –ì—Ä–µ—á–µ—Å–∫–∏–µ –±—É–∫–≤—ã: "–ø–∏", "—Å–∏–≥–º–∞", "–¥–µ–ª—å—Ç–∞" –≤–º–µ—Å—Ç–æ "œÄ", "œÉ", "Œ¥"

–û—Å–æ–±–µ–Ω–Ω–æ—Å—Ç–∏ –≤–∞—à–µ–≥–æ —Å—Ç–∏–ª—è:
- –û–±—ä—è—Å–Ω—è–π—Ç–µ —Å–ª–æ–∂–Ω–æ–µ –ø—Ä–æ—Å—Ç—ã–º–∏ —Å–ª–æ–≤–∞–º–∏
- –ò—Å–ø–æ–ª—å–∑—É–π—Ç–µ –ø—Ä–∏–º–µ—Ä—ã –∏ –∞–Ω–∞–ª–æ–≥–∏–∏
- –ë—É–¥—å—Ç–µ –∫—Ä–∞—Ç–∫–∏, –Ω–æ –∏–Ω—Ñ–æ—Ä–º–∞—Ç–∏–≤–Ω—ã
- –ó–∞–¥–∞–≤–∞–π—Ç–µ –Ω–∞–≤–æ–¥—è—â–∏–µ –≤–æ–ø—Ä–æ—Å—ã –¥–ª—è –ª—É—á—à–µ–≥–æ –ø–æ–Ω–∏–º–∞–Ω–∏—è
- –ê–¥–∞–ø—Ç–∏—Ä—É–π—Ç–µ –æ–±—ä—è—Å–Ω–µ–Ω–∏—è –ø–æ–¥ —É—Ä–æ–≤–µ–Ω—å —É—á–µ–Ω–∏–∫–∞

–ö–†–ò–¢–ò–ß–ù–û –í–ê–ñ–ù–û –î–õ–Ø –ì–û–õ–û–°–û–í–û–ì–û –ß–ê–¢–ê: –ê–ö–¢–ò–í–ù–û –ò–°–ü–û–õ–¨–ó–£–ô–¢–ï –ò–°–¢–û–†–ò–Æ –ë–ï–°–ï–î–´!
- –í—Å–µ–≥–¥–∞ —Å—Å—ã–ª–∞–π—Ç–µ—Å—å –Ω–∞ –ø—Ä–µ–¥—ã–¥—É—â–∏–µ —Å–æ–æ–±—â–µ–Ω–∏—è –≤ —Ä–∞–∑–≥–æ–≤–æ—Ä–µ
- –ü–æ–º–Ω–∏—Ç–µ, —á—Ç–æ –æ–±—Å—É–∂–¥–∞–ª–æ—Å—å —Ä–∞–Ω–µ–µ –≤ —ç—Ç–æ–π –±–µ—Å–µ–¥–µ
- –ü—Ä–æ–¥–æ–ª–∂–∞–π—Ç–µ –ª–æ–≥–∏—á–µ—Å–∫—É—é –Ω–∏—Ç—å —Ä–∞–∑–≥–æ–≤–æ—Ä–∞
- –ò–∑–±–µ–≥–∞–π—Ç–µ –ø–æ–≤—Ç–æ—Ä–µ–Ω–∏–π —É–∂–µ –æ–±—ä—è—Å–Ω–µ–Ω–Ω–æ–≥–æ –º–∞—Ç–µ—Ä–∏–∞–ª–∞
- –ò—Å–ø–æ–ª—å–∑—É–π—Ç–µ —Ñ—Ä–∞–∑—ã —Ç–∏–ø–∞ "–∫–∞–∫ –º—ã —Ç–æ–ª—å–∫–æ —á—Ç–æ –æ–±—Å—É–∂–¥–∞–ª–∏", "–ø—Ä–æ–¥–æ–ª–∂–∞—è –Ω–∞—à—É —Ç–µ–º—É", "–Ω–∞ –æ—Å–Ω–æ–≤–µ –ø—Ä–µ–¥—ã–¥—É—â–µ–≥–æ –æ–±—ä—è—Å–Ω–µ–Ω–∏—è"
- –û—Ç–≤–µ—á–∞–π—Ç–µ –≤ –∫–æ–Ω—Ç–µ–∫—Å—Ç–µ –≤—Å–µ–≥–æ —Ä–∞–∑–≥–æ–≤–æ—Ä–∞, –∞ –Ω–µ –∏–∑–æ–ª–∏—Ä–æ–≤–∞–Ω–Ω–æ

–ü–∞–º—è—Ç—å –∏ –∫–æ–Ω—Ç–µ–∫—Å—Ç: –ò—Å—Ç–æ—Ä–∏—è –ø–æ—Å–ª–µ–¥–Ω–∏—Ö 30 —Å–æ–æ–±—â–µ–Ω–∏–π –ø–µ—Ä–µ–¥–∞–µ—Ç—Å—è –≤–∞–º. –û–ë–Ø–ó–ê–¢–ï–õ–¨–ù–û –∏—Å–ø–æ–ª—å–∑—É–π—Ç–µ –µ—ë!`,
                    },
                    ...messages.slice(-30).map(msg => ({ // Keep last 30 messages for teacher memory context
                      role: msg.role,
                      content: msg.content,
                    })),
                    {
                      role: 'user',
                      content: finalTranscript.trim(),
                    },
                  ],
                  max_tokens: 1000, // Increased for extended teacher memory context with 30 messages
                  temperature: 0.7,
                }),
              });
            } catch (fetchError) {
              console.error('Fetch error:', fetchError);
              throw new Error('–û—à–∏–±–∫–∞ —Å–µ—Ç–∏ –ø—Ä–∏ –ø–æ–¥–∫–ª—é—á–µ–Ω–∏–∏ –∫ OpenAI');
            }

            if (!response.ok) {
              const errorData = await response.json().catch(() => ({}));
              console.error('OpenAI API Error:', {
                status: response.status,
                statusText: response.statusText,
                error: errorData
              });

              // Handle specific error codes
              if (response.status === 401) {
                throw new Error('–ù–µ–≤–µ—Ä–Ω—ã–π API –∫–ª—é—á OpenAI. –ü—Ä–æ–≤–µ—Ä—å—Ç–µ –Ω–∞—Å—Ç—Ä–æ–π–∫–∏.');
              } else if (response.status === 429) {
                throw new Error('–ü—Ä–µ–≤—ã—à–µ–Ω –ª–∏–º–∏—Ç –∑–∞–ø—Ä–æ—Å–æ–≤ –∫ OpenAI. –ü–æ–ø—Ä–æ–±—É–π—Ç–µ –ø–æ–∑–∂–µ.');
              } else if (response.status === 500) {
                throw new Error('–û—à–∏–±–∫–∞ —Å–µ—Ä–≤–µ—Ä–∞ OpenAI. –ü–æ–ø—Ä–æ–±—É–π—Ç–µ –µ—â–µ —Ä–∞–∑.');
              } else {
                throw new Error(`–û—à–∏–±–∫–∞ OpenAI: ${response.status} ${response.statusText}`);
              }
            }

            const data = await response.json();

            if (!data.choices || !data.choices[0] || !data.choices[0].message) {
              console.error('Invalid OpenAI response:', data);
              throw new Error('–ù–µ–∫–æ—Ä—Ä–µ–∫—Ç–Ω—ã–π –æ—Ç–≤–µ—Ç –æ—Ç OpenAI');
            }

            const assistantMessage: Message = {
              id: (Date.now() + 1).toString(),
              role: 'assistant',
              content: data.choices[0].message.content,
              timestamp: new Date(),
            };

            // Mark as TTS played to prevent auto-TTS duplication
            assistantMessage.ttsPlayed = true;

            setMessages(prev => [...prev, assistantMessage]);


            // Speak the response sentence by sentence for faster TTS
            await speakTextBySentences(assistantMessage.content, assistantMessage.id);

          } catch (error) {
            console.error('Voice chat error:', error);
            const errorMessage: Message = {
              id: (Date.now() + 1).toString(),
              role: 'assistant',
              content: '–ò–∑–≤–∏–Ω–∏—Ç–µ, –ø—Ä–æ–∏–∑–æ—à–ª–∞ –æ—à–∏–±–∫–∞. –ü—Ä–æ–¥–æ–ª–∂–∞–π—Ç–µ –≥–æ–≤–æ—Ä–∏—Ç—å.',
              timestamp: new Date(),
            };
            setMessages(prev => [...prev, errorMessage]);
          } finally {
            setIsLoading(false);
            isProcessing = false;
            // Stop continuous thinking sound
            stopContinuousSound();

            // Resume listening after a short delay
            setTimeout(() => {
              if (isVoiceChatActive && recognitionRef.current && !OpenAITTS.isPlaying()) {
                setIsListening(true);
                try {
                  recognitionRef.current.start();
                } catch (e) {
                  // Recognition might already be started, ignore
                }
              }
            }, 500);
          }
        }
      };

      recognitionRef.current.onstart = () => {
        console.log('Speech recognition started');
        setIsListening(true);
      };

      recognitionRef.current.onerror = (event) => {
        console.error('Speech recognition error:', event);
        setIsListening(false);

        // Try to restart listening if it's a non-fatal error
        if (isVoiceChatActive && event.error !== 'not-allowed' && event.error !== 'service-not-allowed') {
          setTimeout(() => {
            if (isVoiceChatActive && recognitionRef.current) {
              try {
                recognitionRef.current.start();
              } catch (e) {
                console.error('Failed to restart recognition:', e);
              }
            }
          }, 1000);
        }
      };

      recognitionRef.current.onend = () => {
        console.log('Speech recognition ended');
        setIsListening(false);

        // Restart listening if voice chat is still active and not speaking
        if (isVoiceChatActive && !OpenAITTS.isPlaying()) {
          setTimeout(() => {
            if (isVoiceChatActive && recognitionRef.current) {
              try {
                setIsListening(true);
                recognitionRef.current.start();
              } catch (e) {
                console.error('Failed to restart recognition:', e);
              }
            }
          }, 300);
        }
      };

      // Start listening
      recognitionRef.current.start();

    } catch (error) {
      console.error('Voice chat start error:', error);
      setIsVoiceChatActive(false);
      setIsListening(false);
      alert('–û—à–∏–±–∫–∞ –∑–∞–ø—É—Å–∫–∞ –≥–æ–ª–æ—Å–æ–≤–æ–≥–æ –æ–±—â–µ–Ω–∏—è.');
    }
  };

  // Function to start camera
  const startCamera = async () => {
    try {
      const stream = await navigator.mediaDevices.getUserMedia({
        video: { facingMode: 'environment' }, // Use back camera if available
        audio: false
      });
      setCameraStream(stream);
      setIsCameraActive(true);

      if (videoRef.current) {
        videoRef.current.srcObject = stream;
      }
    } catch (error) {
      console.error('Error accessing camera:', error);
      alert('–ù–µ —É–¥–∞–ª–æ—Å—å –ø–æ–ª—É—á–∏—Ç—å –¥–æ—Å—Ç—É–ø –∫ –∫–∞–º–µ—Ä–µ. –ü—Ä–æ–≤–µ—Ä—å—Ç–µ —Ä–∞–∑—Ä–µ—à–µ–Ω–∏—è.');
    }
  };

  // Function to stop camera
  const stopCamera = () => {
    if (cameraStream) {
      cameraStream.getTracks().forEach(track => track.stop());
      setCameraStream(null);
    }
    setIsCameraActive(false);
  };

  // Function to capture photo
  const capturePhoto = () => {
    if (!videoRef.current || !canvasRef.current) return;

    const video = videoRef.current;
    const canvas = canvasRef.current;
    const context = canvas.getContext('2d');

    if (!context) return;

    // Set canvas size to video size
    canvas.width = video.videoWidth;
    canvas.height = video.videoHeight;

    // Draw video frame to canvas
    context.drawImage(video, 0, 0, canvas.width, canvas.height);

    // Convert to base64
    const imageData = canvas.toDataURL('image/jpeg', 0.8);
    setCapturedImage(imageData);
    stopCamera();
  };

  // Function to retake photo
  const retakePhoto = () => {
    setCapturedImage(null);
    startCamera();
  };

  // Function to send captured photo as task
  const sendCapturedPhoto = async () => {
    if (!capturedImage) return;

    const userMessage: Message = {
      id: Date.now().toString(),
      role: 'user',
      content: '–Ø —Å—Ñ–æ—Ç–æ–≥—Ä–∞—Ñ–∏—Ä–æ–≤–∞–ª –∑–∞–¥–∞—á—É. –ü–æ–∂–∞–ª—É–π—Å—Ç–∞, —Ä–µ—à–∏ –µ—ë.',
      timestamp: new Date(),
    };

    setMessages(prev => [...prev, userMessage]);
    setIsLoading(true);

    try {
      // Send photo to OpenAI Vision API for task recognition and solving
      const response = await fetch(`${window.location.origin}/api/chat/completions`, {
        method: 'POST',
        headers: {
          'Content-Type': 'application/json',
        },
        body: JSON.stringify({
          model: 'gpt-4o-mini',
          messages: [
            {
              role: 'system',
              content: `–¢—ã - –æ–ø—ã—Ç–Ω—ã–π –ø—Ä–µ–ø–æ–¥–∞–≤–∞—Ç–µ–ª—å –∏ —Ä–µ—à–∞—Ç–µ–ª—å –∑–∞–¥–∞—á. –ü–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—å –ø—Ä–∏—Å–ª–∞–ª —Ñ–æ—Ç–æ–≥—Ä–∞—Ñ–∏—é –∑–∞–¥–∞—á–∏. 
              
–í–ê–ñ–ù–´–ï –ò–ù–°–¢–†–£–ö–¶–ò–ò:
1. –ü—Ä–æ–∞–Ω–∞–ª–∏–∑–∏—Ä—É–π –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–µ –∏ —Ä–∞—Å–ø–æ–∑–Ω–∞–π —É—á–µ–±–Ω—É—é –∑–∞–¥–∞—á—É
2. –ü–æ–∫–∞–∂–∏ –ø–æ—à–∞–≥–æ–≤–æ–µ —Ä–µ—à–µ–Ω–∏–µ –∑–∞–¥–∞—á–∏
3. –û–±—ä—è—Å–Ω–∏ –∫–∞–∂–¥—ã–π —à–∞–≥ –ø–æ–¥—Ä–æ–±–Ω–æ
4. –ï—Å–ª–∏ –∑–∞–¥–∞—á–∞ –º–∞—Ç–µ–º–∞—Ç–∏—á–µ—Å–∫–∞—è - –ø–æ–∫–∞–∂–∏ –≤—Å–µ –≤—ã—á–∏—Å–ª–µ–Ω–∏—è
5. –ï—Å–ª–∏ –∑–∞–¥–∞—á–∞ —Ç–µ–∫—Å—Ç–æ–≤–∞—è - —Ä–∞–∑–±–µ—Ä–∏ –µ—ë —Å—Ç—Ä—É–∫—Ç—É—Ä—É
6. –î–∞–π –æ–∫–æ–Ω—á–∞—Ç–µ–ª—å–Ω—ã–π –æ—Ç–≤–µ—Ç
7. –ï—Å–ª–∏ –Ω–µ –º–æ–∂–µ—à—å —Ä–∞—Å–ø–æ–∑–Ω–∞—Ç—å –∑–∞–¥–∞—á—É - –ø–æ–ø—Ä–æ—Å–∏ –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—è —Å–¥–µ–ª–∞—Ç—å –±–æ–ª–µ–µ –∫–∞—á–µ—Å—Ç–≤–µ–Ω–Ω–æ–µ —Ñ–æ—Ç–æ

–§–æ—Ä–º–∞—Ç –æ—Ç–≤–µ—Ç–∞:
1. **–†–∞—Å–ø–æ–∑–Ω–∞–Ω–Ω–∞—è –∑–∞–¥–∞—á–∞**: [—á—Ç–æ —Ç—ã —É–≤–∏–¥–µ–ª –Ω–∞ —Ñ–æ—Ç–æ]
2. **–†–µ—à–µ–Ω–∏–µ**:
   - –®–∞–≥ 1: [–ø–µ—Ä–≤—ã–π —à–∞–≥]
   - –®–∞–≥ 2: [–≤—Ç–æ—Ä–æ–π —à–∞–≥]
   ...
3. **–û—Ç–≤–µ—Ç**: [—Ñ–∏–Ω–∞–ª—å–Ω—ã–π —Ä–µ–∑—É–ª—å—Ç–∞—Ç]`,
            },
            {
              role: 'user',
              content: [
                {
                  type: 'text',
                  text: '–†–µ—à–∏ –∑–∞–¥–∞—á—É –Ω–∞ —ç—Ç–æ–π —Ñ–æ—Ç–æ–≥—Ä–∞—Ñ–∏–∏. –ü–æ–∫–∞–∂–∏ –ø–æ–¥—Ä–æ–±–Ω–æ–µ —Ä–µ—à–µ–Ω–∏–µ.'
                },
                {
                  type: 'image_url',
                  image_url: {
                    url: capturedImage
                  }
                }
              ]
            },
          ],
          max_tokens: 2000,
          temperature: 0.1,
        }),
      });

      if (!response.ok) {
        throw new Error('Failed to analyze photo task');
      }

      const data = await response.json();
      const assistantMessage: Message = {
        id: Date.now().toString(),
        role: 'assistant',
        content: data.choices[0].message.content,
        timestamp: new Date(),
      };
      setMessages(prev => [...prev, assistantMessage]);
    } catch (error) {
      console.error('Error analyzing photo task:', error);
      setMessages(prev => [
        ...prev,
        {
          id: Date.now().toString(),
          role: 'assistant',
          content: '–ò–∑–≤–∏–Ω–∏—Ç–µ, –ø—Ä–æ–∏–∑–æ—à–ª–∞ –æ—à–∏–±–∫–∞ –ø—Ä–∏ –∞–Ω–∞–ª–∏–∑–µ —Ñ–æ—Ç–æ–≥—Ä–∞—Ñ–∏–∏ –∑–∞–¥–∞—á–∏. –ü–æ–ø—Ä–æ–±—É–π—Ç–µ –µ—â—ë —Ä–∞–∑.',
          timestamp: new Date(),
        },
      ]);
    } finally {
      setIsLoading(false);
      setCapturedImage(null);
      if (inputRef.current) {
        inputRef.current.focus();
      }
    }
  };


  // Function to handle file upload
  const handleFileUpload = (event: React.ChangeEvent<HTMLInputElement>) => {
    const files = Array.from(event.target.files || []);
    if (files.length > 0) {
      setUploadedFiles(prev => [...prev, ...files]);
    }
  };

  // Function to remove uploaded file
  const removeFile = (index: number) => {
    setUploadedFiles(prev => prev.filter((_, i) => i !== index));
  };

  // Function to get file icon based on type
  const getFileIcon = (file: File) => {
    const type = file.type;
    if (type.startsWith('image/')) return <Image className="w-4 h-4" />;
    if (type === 'application/pdf') return <FileText className="w-4 h-4" />;
    if (type.includes('word') || type.includes('document')) return <FileText className="w-4 h-4" />;
    return <File className="w-4 h-4" />;
  };

  // Function to extract text from images using OpenAI Vision API
  const extractTextFromImage = async (file: File): Promise<string> => {
    return new Promise((resolve, reject) => {
      const reader = new FileReader();
      reader.onload = async () => {
        try {
          const base64Image = reader.result as string;

          const response = await fetch(`${window.location.origin}/api/chat/completions`, {
            method: 'POST',
            headers: {
              'Content-Type': 'application/json',
            },
            body: JSON.stringify({
              model: 'gpt-4o-mini',
              messages: [
                {
                  role: 'user',
                  content: [
                    {
                      type: 'text',
                      text: '–†–∞—Å–ø–æ–∑–Ω–∞–π –≤–µ—Å—å —Ç–µ–∫—Å—Ç –Ω–∞ —ç—Ç–æ–º –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–∏. –í–µ—Ä–Ω–∏ —Ç–æ–ª—å–∫–æ —Ä–∞—Å–ø–æ–∑–Ω–∞–Ω–Ω—ã–π —Ç–µ–∫—Å—Ç –±–µ–∑ –¥–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω—ã—Ö –∫–æ–º–º–µ–Ω—Ç–∞—Ä–∏–µ–≤.'
                    },
                    {
                      type: 'image_url',
                      image_url: {
                        url: base64Image
                      }
                    }
                  ]
                }
              ],
              max_tokens: 1000,
              temperature: 0.1,
            }),
          });

          if (!response.ok) {
            throw new Error('Failed to process image with OpenAI');
          }

          const data = await response.json();
          const extractedText = data.choices[0].message.content.trim();

          resolve(extractedText || `–ù–µ —É–¥–∞–ª–æ—Å—å —Ä–∞—Å–ø–æ–∑–Ω–∞—Ç—å —Ç–µ–∫—Å—Ç –≤ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–∏ ${file.name}.`);
        } catch (error) {
          console.error('OCR Error:', error);
          reject(new Error(`–û—à–∏–±–∫–∞ –ø—Ä–∏ —Ä–∞—Å–ø–æ–∑–Ω–∞–≤–∞–Ω–∏–∏ —Ç–µ–∫—Å—Ç–∞: ${error.message}`));
        }
      };

      reader.onerror = () => reject(new Error('Failed to read file'));
      reader.readAsDataURL(file);
    });
  };

  // Function to extract text from PDF using PDF.js
  const extractTextFromPDF = async (file: File): Promise<string> => {
    try {
      // Dynamic import of PDF.js to avoid bundle bloat
      const pdfjsLib = await import('pdfjs-dist');

      // Set worker source
      pdfjsLib.GlobalWorkerOptions.workerSrc = `//cdnjs.cloudflare.com/ajax/libs/pdf.js/${pdfjsLib.version}/pdf.worker.min.js`;

      const arrayBuffer = await file.arrayBuffer();
      const pdf = await pdfjsLib.getDocument({ data: arrayBuffer }).promise;

      let fullText = '';

      // Extract text from each page
      for (let pageNum = 1; pageNum <= pdf.numPages; pageNum++) {
        const page = await pdf.getPage(pageNum);
        const textContent = await page.getTextContent();

        const pageText = textContent.items
          .map((item: any) => item.str)
          .join(' ');

        fullText += pageText + '\n\n';
      }

      return fullText.trim() || `–ù–µ —É–¥–∞–ª–æ—Å—å –∏–∑–≤–ª–µ—á—å —Ç–µ–∫—Å—Ç –∏–∑ PDF —Ñ–∞–π–ª–∞ "${file.name}". –í–æ–∑–º–æ–∂–Ω–æ, —Ñ–∞–π–ª —Å–æ–¥–µ—Ä–∂–∏—Ç —Ç–æ–ª—å–∫–æ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è –∏–ª–∏ –∏–º–µ–µ—Ç —Å–ª–æ–∂–Ω—É—é —Å—Ç—Ä—É–∫—Ç—É—Ä—É.`;

    } catch (error) {
      console.error('PDF processing error:', error);
      return `–û—à–∏–±–∫–∞ –ø—Ä–∏ –æ–±—Ä–∞–±–æ—Ç–∫–µ PDF —Ñ–∞–π–ª–∞ "${file.name}": ${error.message}. –ü–æ–ø—Ä–æ–±—É–π—Ç–µ —Å–∫–æ–ø–∏—Ä–æ–≤–∞—Ç—å —Ç–µ–∫—Å—Ç –≤—Ä—É—á–Ω—É—é –∏–∑ —Ñ–∞–π–ª–∞.`;
    }
  };

  // Function to extract text from DOCX
  const extractTextFromDOCX = async (file: File): Promise<string> => {
    // For browser environment, we'll use a temporary approach
    // In production, you'd need a server-side solution or a browser-compatible DOCX library
    return new Promise((resolve) => {
      setTimeout(() => {
        resolve(`DOCX —Ñ–∞–π–ª "${file.name}" –ø—Ä–∏–Ω—è—Ç –¥–ª—è –æ–±—Ä–∞–±–æ—Ç–∫–∏. –ò–∑–≤–ª–µ—á–µ–Ω–∏–µ —Ç–µ–∫—Å—Ç–∞ –∏–∑ Word –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤ —Ç—Ä–µ–±—É–µ—Ç —Å–µ—Ä–≤–µ—Ä–Ω–æ–π –æ–±—Ä–∞–±–æ—Ç–∫–∏. –í —Ä–µ–∞–ª—å–Ω–æ–º –ø—Ä–∏–ª–æ–∂–µ–Ω–∏–∏ –∑–¥–µ—Å—å –±—É–¥–µ—Ç —Å–æ–¥–µ—Ä–∂–∏–º–æ–µ –¥–æ–∫—É–º–µ–Ω—Ç–∞, –∏–∑–≤–ª–µ—á–µ–Ω–Ω–æ–µ –Ω–∞ —Å–µ—Ä–≤–µ—Ä–µ.`);
      }, 1500);
    });
  };

  // Function to process file and extract text
  const processFile = async (file: File): Promise<string> => {
    const fileType = file.type;

    if (fileType.startsWith('image/')) {
      return await extractTextFromImage(file);
    } else if (fileType === 'application/pdf') {
      return await extractTextFromPDF(file);
    } else if (fileType.includes('word') || fileType.includes('document')) {
      return await extractTextFromDOCX(file);
    } else {
      return `–§–∞–π–ª ${file.name} –Ω–µ –ø–æ–¥–¥–µ—Ä–∂–∏–≤–∞–µ—Ç—Å—è –¥–ª—è OCR.`;
    }
  };

  // Function to parse **bold green text** within a string
  const parseBoldGreenText = (text: string, keyPrefix: string) => {
    const parts = text.split(/(\*\*.*?\*\*)/g);
    return parts.map((part, index) => {
      if (part.startsWith('**') && part.endsWith('**')) {
        // This is a **bold green text** block
        const greenText = part.slice(2, -2); // Remove ** from both sides
        return (
          <span key={`${keyPrefix}-green-${index}`} className="text-green-600 font-semibold">
            {greenText}
          </span>
        );
      }
      // Regular text
      return part;
    });
  };

  // Function to parse and format message content with ### blocks and **green text**
  const formatMessageContent = (content: string) => {
    // Split by lines to process each line individually
    const lines = content.split('\n');
    const result: JSX.Element[] = [];
    let currentBlock: string[] = [];

    lines.forEach((line, index) => {
      if (line.trim().startsWith('### ')) {
        // If we were in a regular block, close it first
        if (currentBlock.length > 0) {
          const blockText = currentBlock.join('\n');
          result.push(
            <span key={`text-${result.length}`} className="whitespace-pre-wrap">
              {parseBoldGreenText(blockText, `text-${result.length}`)}
            </span>
          );
          currentBlock = [];
        }

        // Start or continue a ### block
        const blockContent = line.trim().replace(/^### /, '');
        result.push(
          <div key={`block-${result.length}`} className="text-green-600 font-bold text-lg my-2 bg-green-50 dark:bg-green-950/20 px-3 py-2 rounded-md border-l-4 border-green-500">
            {parseBoldGreenText(blockContent, `block-${result.length}`)}
          </div>
        );
      } else {
        // Regular line
        currentBlock.push(line);
      }
    });

    // Add any remaining regular text
    if (currentBlock.length > 0) {
      const blockText = currentBlock.join('\n');
      result.push(
        <span key={`text-${result.length}`} className="whitespace-pre-wrap">
          {parseBoldGreenText(blockText, `text-${result.length}`)}
        </span>
      );
    }

    return result.length > 0 ? result : [<span key="empty" className="whitespace-pre-wrap">{parseBoldGreenText(content, 'empty')}</span>];
  };

  // Redirect to auth if not authenticated
  useEffect(() => {
    if (!isAuthenticated) {
      navigate('/auth');
    }
  }, [isAuthenticated, navigate]);

  // Auto scroll to bottom when new messages arrive
  useEffect(() => {
    if (scrollAreaRef.current) {
      scrollAreaRef.current.scrollTop = scrollAreaRef.current.scrollHeight;
    }
  }, [messages]);

  // Focus input on mount
  useEffect(() => {
    if (inputRef.current) {
      inputRef.current.focus();
    }
  }, []);

  // Check OpenAI API key on mount
  useEffect(() => {
    const checkApiKey = async () => {
      const apiKey = import.meta.env.VITE_OPENAI_API_KEY;
      if (!apiKey) {
        setApiKeyStatus('invalid');
        return;
      }

      try {
        // Make a minimal request to check if API key is valid
        const response = await fetch(`${window.location.origin}/api/models`, {
          method: 'GET',
          headers: {
            'Authorization': `Bearer ${apiKey}`,
          },
        });

        if (response.ok) {
          setApiKeyStatus('valid');
        } else if (response.status === 401) {
          setApiKeyStatus('invalid');
        } else {
          setApiKeyStatus('error');
        }
      } catch (error) {
        console.error('API key check failed:', error);
        setApiKeyStatus('error');
      }
    };

    checkApiKey();
  }, []);

  // Global keyboard shortcuts for TTS control
  useEffect(() => {
    const handleKeyDown = (event: KeyboardEvent) => {
      // Escape or Space to stop TTS
      if ((event.key === 'Escape' || event.key === ' ') && OpenAITTS.isPlaying()) {
        event.preventDefault();
        OpenAITTS.stop();
        console.log('üõë TTS stopped by keyboard shortcut');
      }
    };

    document.addEventListener('keydown', handleKeyDown);
    return () => document.removeEventListener('keydown', handleKeyDown);
  }, []);


  const clearMemory = () => {
    // Keep only the first system message, clear all user/assistant messages
    setMessages(prev => prev.filter(msg => msg.role === 'system'));
    // Stop any ongoing TTS and sounds
    OpenAITTS.stop();
    stopContinuousSound();
    if (currentAudioRef.current) {
      currentAudioRef.current.pause();
      currentAudioRef.current = null;
    }

    ttsContinueRef.current = true;
    setSpeakingMessageId(null);
    setCurrentSentence(0);
    setTotalSentences(0);
    setIsGeneratingTTS(false);
  };

  const sendMessage = async () => {
    if ((!inputMessage.trim() && uploadedFiles.length === 0) || isLoading || isProcessingFile) return;

    // Stop any ongoing TTS and sounds when user sends a new message
    if (OpenAITTS.isPlaying()) {
      OpenAITTS.stop();
      console.log('üõë TTS stopped due to new user message');
    }
    stopContinuousSound();

    let messageContent = inputMessage;
    let fileContents: string[] = [];

    // Process uploaded files first
    if (uploadedFiles.length > 0) {
      setIsProcessingFile(true);
      try {
        for (const file of uploadedFiles) {
          const extractedText = await processFile(file);
          fileContents.push(`\n\n--- –°–æ–¥–µ—Ä–∂–∏–º–æ–µ —Ñ–∞–π–ª–∞ ${file.name} ---\n${extractedText}`);
        }
        messageContent += fileContents.join('\n');
      } catch (error) {
        console.error('Error processing files:', error);
        messageContent += '\n\n–û—à–∏–±–∫–∞ –ø—Ä–∏ –æ–±—Ä–∞–±–æ—Ç–∫–µ —Ñ–∞–π–ª–æ–≤.';
      } finally {
        setIsProcessingFile(false);
      }
    }

    const userMessage: Message = {
      id: Date.now().toString(),
      role: 'user',
      content: messageContent,
      timestamp: new Date(),
    };

    setMessages(prev => [...prev, userMessage]);
    setInputMessage('');
    setUploadedFiles([]); // Clear uploaded files after sending
    setIsLoading(true);

    try {
      const response = await fetch(`${window.location.origin}/api/chat/completions`, {
        method: 'POST',
        headers: {
          'Content-Type': 'application/json',
        },
        body: JSON.stringify({
          model: 'gpt-4o-mini',
          messages: [
            {
              role: 'system',
              content: `–í—ã - –ø—Ä–æ—Ñ–µ—Å—Å–∏–æ–Ω–∞–ª—å–Ω—ã–π –ø–µ–¥–∞–≥–æ–≥ –∏ —ç–∫—Å–ø–µ—Ä—Ç –≤ –æ–±—Ä–∞–∑–æ–≤–∞–Ω–∏–∏. –í–∞—à–∞ –∑–∞–¥–∞—á–∞ - –æ–±—ä—è—Å–Ω—è—Ç—å –ª—é–±—ã–µ —Ç–µ–º—ã –±—ã—Å—Ç—Ä–æ, –ø–æ–Ω—è—Ç–Ω–æ –∏ –¥–æ—Å—Ç—É–ø–Ω–æ. –í—ã –º–æ–∂–µ—Ç–µ "—Ä–∞–∑–∂–µ–≤—ã–≤–∞—Ç—å" —Å–ª–æ–∂–Ω—ã–µ –∫–æ–Ω—Ü–µ–ø—Ü–∏–∏, –ø—Ä–∏–≤–æ–¥–∏—Ç—å –ø—Ä–∏–º–µ—Ä—ã –∏–∑ —Ä–µ–∞–ª—å–Ω–æ–π –∂–∏–∑–Ω–∏, –∏—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å –∞–Ω–∞–ª–æ–≥–∏–∏ –∏ –ø–æ—à–∞–≥–æ–≤—ã–µ –æ–±—ä—è—Å–Ω–µ–Ω–∏—è.

–û—Å–æ–±–µ–Ω–Ω–æ—Å—Ç–∏ –≤–∞—à–µ–≥–æ —Å—Ç–∏–ª—è:
- –û–±—ä—è—Å–Ω—è–π—Ç–µ —Å–ª–æ–∂–Ω–æ–µ –ø—Ä–æ—Å—Ç—ã–º–∏ —Å–ª–æ–≤–∞–º–∏
- –ò—Å–ø–æ–ª—å–∑—É–π—Ç–µ –ø—Ä–∏–º–µ—Ä—ã –∏ –∞–Ω–∞–ª–æ–≥–∏–∏
- –†–∞–∑–±–∏–≤–∞–π—Ç–µ –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—é –Ω–∞ –ª–æ–≥–∏—á–µ—Å–∫–∏–µ –±–ª–æ–∫–∏
- –ó–∞–¥–∞–≤–∞–π—Ç–µ –Ω–∞–≤–æ–¥—è—â–∏–µ –≤–æ–ø—Ä–æ—Å—ã –¥–ª—è –ª—É—á—à–µ–≥–æ –ø–æ–Ω–∏–º–∞–Ω–∏—è
- –ë—É–¥—å—Ç–µ —Ç–µ—Ä–ø–µ–ª–∏–≤—ã –∏ –ø–æ–¥–¥–µ—Ä–∂–∏–≤–∞—é—â–∏
- –ê–¥–∞–ø—Ç–∏—Ä—É–π—Ç–µ –æ–±—ä—è—Å–Ω–µ–Ω–∏—è –ø–æ–¥ —É—Ä–æ–≤–µ–Ω—å —É—á–µ–Ω–∏–∫–∞
- –ü–æ–æ—â—Ä—è–π—Ç–µ —Å–∞–º–æ—Å—Ç–æ—è—Ç–µ–ª—å–Ω–æ–µ –º—ã—à–ª–µ–Ω–∏–µ

–ö–†–ò–¢–ò–ß–ù–û –í–ê–ñ–ù–û: –ê–ö–¢–ò–í–ù–û –ò–°–ü–û–õ–¨–ó–£–ô–¢–ï –ò–°–¢–û–†–ò–Æ –ë–ï–°–ï–î–´!
- –í—Å–µ–≥–¥–∞ —Å—Å—ã–ª–∞–π—Ç–µ—Å—å –Ω–∞ –ø—Ä–µ–¥—ã–¥—É—â–∏–µ —Å–æ–æ–±—â–µ–Ω–∏—è
- –ü–æ–º–Ω–∏—Ç–µ, —á—Ç–æ –æ–±—Å—É–∂–¥–∞–ª–æ—Å—å —Ä–∞–Ω–µ–µ
- –ü—Ä–æ–¥–æ–ª–∂–∞–π—Ç–µ –ª–æ–≥–∏—á–µ—Å–∫—É—é –Ω–∏—Ç—å —Ä–∞–∑–≥–æ–≤–æ—Ä–∞
- –ò–∑–±–µ–≥–∞–π—Ç–µ –ø–æ–≤—Ç–æ—Ä–µ–Ω–∏–π —É–∂–µ –æ–±—ä—è—Å–Ω–µ–Ω–Ω–æ–≥–æ
- –ò—Å–ø–æ–ª—å–∑—É–π—Ç–µ —Ñ—Ä–∞–∑—ã —Ç–∏–ø–∞ "–∫–∞–∫ –º—ã –æ–±—Å—É–∂–¥–∞–ª–∏ —Ä–∞–Ω–µ–µ", "–ø—Ä–æ–¥–æ–ª–∂–∞—è –Ω–∞—à—É —Ç–µ–º—É", "–Ω–∞ –æ—Å–Ω–æ–≤–µ –ø—Ä–µ–¥—ã–¥—É—â–µ–≥–æ –æ–±—ä—è—Å–Ω–µ–Ω–∏—è"

–ü–æ–º–Ω–∏—Ç–µ: –≤–∞—à–∞ —Ü–µ–ª—å - –Ω–µ –ø—Ä–æ—Å—Ç–æ –¥–∞—Ç—å –æ—Ç–≤–µ—Ç, –∞ –ø–æ–¥–¥–µ—Ä–∂–∏–≤–∞—Ç—å –Ω–µ–ø—Ä–µ—Ä—ã–≤–Ω—ã–π –æ–±—Ä–∞–∑–æ–≤–∞—Ç–µ–ª—å–Ω—ã–π –¥–∏–∞–ª–æ–≥!`,
            },
            ...messages.slice(-29).map(msg => ({
              role: msg.role,
              content: msg.content,
            })),
            {
              role: 'user',
              content: userMessage.content,
            },
          ],
          max_tokens: 1000,
          temperature: 0.7,
        }),
      });

      if (!response.ok) {
        const errorData = await response.json().catch(() => ({}));
        console.error('OpenAI API Error:', {
          status: response.status,
          statusText: response.statusText,
          error: errorData
        });

        // Handle specific error codes
        if (response.status === 401) {
          throw new Error('–ù–µ–≤–µ—Ä–Ω—ã–π API –∫–ª—é—á OpenAI. –ü—Ä–æ–≤–µ—Ä—å—Ç–µ –Ω–∞—Å—Ç—Ä–æ–π–∫–∏.');
        } else if (response.status === 429) {
          throw new Error('–ü—Ä–µ–≤—ã—à–µ–Ω –ª–∏–º–∏—Ç –∑–∞–ø—Ä–æ—Å–æ–≤ –∫ OpenAI. –ü–æ–ø—Ä–æ–±—É–π—Ç–µ –ø–æ–∑–∂–µ.');
        } else if (response.status === 500) {
          throw new Error('–û—à–∏–±–∫–∞ —Å–µ—Ä–≤–µ—Ä–∞ OpenAI. –ü–æ–ø—Ä–æ–±—É–π—Ç–µ –µ—â–µ —Ä–∞–∑.');
        } else {
          throw new Error(`–û—à–∏–±–∫–∞ OpenAI: ${response.status} ${response.statusText}`);
        }
      }

      const data = await response.json();

      if (!data.choices || !data.choices[0] || !data.choices[0].message) {
        console.error('Invalid OpenAI response:', data);
        throw new Error('–ù–µ–∫–æ—Ä—Ä–µ–∫—Ç–Ω—ã–π –æ—Ç–≤–µ—Ç –æ—Ç OpenAI');
      }
      const assistantMessage: Message = {
        id: (Date.now() + 1).toString(),
        role: 'assistant',
        content: data.choices[0].message.content,
        timestamp: new Date(),
      };

      setMessages(prev => [...prev, assistantMessage]);
    } catch (error) {
      console.error('Error sending message:', error);
      const errorMessage: Message = {
        id: (Date.now() + 1).toString(),
        role: 'assistant',
        content: '–ò–∑–≤–∏–Ω–∏—Ç–µ, –ø—Ä–æ–∏–∑–æ—à–ª–∞ –æ—à–∏–±–∫–∞ –ø—Ä–∏ –æ–±—Ä–∞–±–æ—Ç–∫–µ –≤–∞—à–µ–≥–æ —Å–æ–æ–±—â–µ–Ω–∏—è. –ü–æ–ø—Ä–æ–±—É–π—Ç–µ –µ—â–µ —Ä–∞–∑.',
        timestamp: new Date(),
      };
      setMessages(prev => [...prev, errorMessage]);
    } finally {
      setIsLoading(false);
      // Focus back to input after response
      setTimeout(() => {
        if (inputRef.current) {
          inputRef.current.focus();
        }
      }, 100);
    }
  };

  const handleKeyPress = (e: React.KeyboardEvent) => {
    if (e.key === 'Enter' && !e.shiftKey) {
      e.preventDefault();
      sendMessage();
    }
  };

  if (!isAuthenticated) {
    return null; // Will redirect to auth
  }


  // Animated Sphere Component
  const AnimatedSphere = () => (
    <div className="min-h-screen bg-gradient-to-br from-background via-secondary/30 to-background flex items-center justify-center">
      <div className="relative">
        {/* Main sphere */}
        <div className="w-32 h-32 bg-gradient-to-r from-blue-500 to-purple-600 rounded-full animate-pulse shadow-2xl relative overflow-hidden">
          <div className="absolute inset-0 bg-gradient-to-r from-transparent via-white/20 to-transparent animate-pulse"></div>
          <div className="absolute inset-2 bg-gradient-to-r from-purple-600 to-blue-500 rounded-full opacity-80 animate-ping"></div>
          <div className="absolute inset-4 bg-gradient-to-r from-blue-500 to-purple-600 rounded-full opacity-60 animate-bounce"></div>
        </div>

        {/* Orbiting elements */}
        <div className="absolute inset-0 animate-spin" style={{ animationDuration: '3s' }}>
          <div className="w-2 h-2 bg-yellow-400 rounded-full absolute top-0 left-1/2 transform -translate-x-1/2 -translate-y-1"></div>
          <div className="w-2 h-2 bg-green-400 rounded-full absolute bottom-0 left-1/2 transform -translate-x-1/2 translate-y-1"></div>
        </div>

        {/* Calm orbiting elements */}
        <div className="absolute inset-0" style={{
          animation: 'gentle-orbit 8s linear infinite'
        }}>
          <div className="w-2 h-2 bg-gradient-to-br from-blue-300 to-blue-400 rounded-full absolute opacity-70 shadow-lg"
               style={{
                 left: '50%',
                 top: '50%',
                 transform: 'translate(-50%, -50%) translateX(-80px)',
                 animation: 'float-glow 4s ease-in-out infinite alternate'
               }}>
          </div>
        </div>

        <div className="absolute inset-0" style={{
          animation: 'gentle-orbit 8s linear infinite reverse'
        }}>
          <div className="w-1.8 h-1.8 bg-gradient-to-br from-purple-300 to-pink-300 rounded-full absolute opacity-60 shadow-lg"
               style={{
                 left: '50%',
                 top: '50%',
                 transform: 'translate(-50%, -50%) translateX(75px)',
                 animation: 'float-glow 3.5s ease-in-out infinite alternate reverse'
               }}>
          </div>
        </div>

        {/* Back to chat button */}
        <Button
          onClick={() => setShowSphere(false)}
          className="absolute -bottom-16 left-1/2 transform -translate-x-1/2 mt-4"
          variant="outline"
        >
          <ArrowLeft className="w-4 h-4 mr-2" />
          –í–µ—Ä–Ω—É—Ç—å—Å—è –∫ —á–∞—Ç—É
        </Button>
      </div>
    </div>
  );

  if (showSphere) {
    return (
      <div className="min-h-screen bg-gradient-to-br from-background via-secondary/30 to-background flex items-center justify-center">
        <AnimatedSphere />
      </div>
    );
  }

  return (
    <div className="min-h-screen bg-gradient-to-br from-background via-secondary/30 to-background">

      {/* Header */}
      <header className="border-b border-border/50 bg-card/50 backdrop-blur-sm">
        <div className="container mx-auto px-4 py-4">
          <div className="flex items-center justify-between gap-2">
            {/* Left side - Logo */}
            <div className="flex items-center gap-2 flex-shrink-0">
              <Button
                variant="ghost"
                onClick={() => navigate('/courses')}
                className="flex items-center gap-2 px-2"
              >
                <ArrowLeft className="w-4 h-4" />
                <span className="hidden sm:inline">–ù–∞–∑–∞–¥</span>
              </Button>
              <div className="w-8 h-8 bg-gradient-to-r from-primary to-accent rounded-xl flex items-center justify-center">
                <Brain className="w-4 h-4 text-white" />
              </div>
              <h1 className="text-sm sm:text-lg font-semibold hidden sm:block">Windexs-–£—á–∏—Ç–µ–ª—å</h1>
            </div>

            {/* Right side - Title */}
            <div className="flex items-center gap-2 flex-shrink-0">
              <MessageCircle className="w-5 h-5 text-primary" />
              <h2 className="text-sm sm:text-lg font-medium hidden sm:block">AI –£—á–∏—Ç–µ–ª—å</h2>
            </div>
          </div>
        </div>
      </header>

      {/* Chat Container */}
      <div className="container mx-auto px-4 py-6 max-w-4xl">
        <Card className="h-[calc(100vh-12rem)]">
          <CardHeader className="pb-4">
            <CardTitle className="flex items-center gap-2">
              <Brain className="w-5 h-5 text-primary" />
              –ß–∞—Ç —Å AI –£—á–∏—Ç–µ–ª–µ–º
            </CardTitle>
            <p className="text-sm text-muted-foreground">
              {apiKeyStatus === 'invalid' && (
                <span className="text-red-600 font-medium block mt-1">
                  ‚ùå OpenAI API –∫–ª—é—á –Ω–µ –Ω–∞—Å—Ç—Ä–æ–µ–Ω! –î–æ–±–∞–≤—å—Ç–µ VITE_OPENAI_API_KEY –≤ —Ñ–∞–π–ª .env
                </span>
              )}
              {apiKeyStatus === 'error' && (
                <span className="text-orange-600 font-medium block mt-1">
                  ‚ö†Ô∏è –ù–µ —É–¥–∞–ª–æ—Å—å –ø—Ä–æ–≤–µ—Ä–∏—Ç—å API –∫–ª—é—á. –í–æ–∑–º–æ–∂–Ω–æ, –ø—Ä–æ–±–ª–µ–º—ã —Å –∏–Ω—Ç–µ—Ä–Ω–µ—Ç–æ–º.
                </span>
              )}
            </p>
          </CardHeader>

          <CardContent className="flex flex-col h-full">
            {/* Messages Area */}
            <ScrollArea className="flex-1 pr-4 mb-4" ref={scrollAreaRef}>
              <div className="space-y-4">
                {messages.length === 0 && (
                  <div className="text-center py-8 text-muted-foreground">
                    <Brain className="w-12 h-12 mx-auto mb-4 text-primary/50" />
                    <p className="text-lg mb-2">–î–æ–±—Ä–æ –ø–æ–∂–∞–ª–æ–≤–∞—Ç—å –≤ —á–∞—Ç —Å AI –£—á–∏—Ç–µ–ª–µ–º!</p>
                    <p>–ó–∞–¥–∞–π—Ç–µ —Å–≤–æ–π –ø–µ—Ä–≤—ã–π –≤–æ–ø—Ä–æ—Å, –∏ —è –ø–æ–º–æ–≥—É —Ä–∞–∑–æ–±—Ä–∞—Ç—å—Å—è –≤ –ª—é–±–æ–π —Ç–µ–º–µ.</p>
                    <div className="mt-4 space-y-2 text-sm">
                      <p><strong>–ü—Ä–∏–º–µ—Ä—ã –≤–æ–ø—Ä–æ—Å–æ–≤:</strong></p>
                      <div className="space-y-1 text-left max-w-md mx-auto">
                        <p>‚Ä¢ "–û–±—ä—è—Å–Ω–∏, —á—Ç–æ —Ç–∞–∫–æ–µ –ø—Ä–æ–∏–∑–≤–æ–¥–Ω–∞—è"</p>
                        <p>‚Ä¢ "–ö–∞–∫ —Ä–∞–±–æ—Ç–∞–µ—Ç —Ñ–æ—Ç–æ—Å–∏–Ω—Ç–µ–∑?"</p>
                        <p>‚Ä¢ "–†–∞—Å—Å–∫–∞–∂–∏ –ø—Ä–æ –≤—Ç–æ—Ä—É—é –º–∏—Ä–æ–≤—É—é –≤–æ–π–Ω—É"</p>
                      </div>
                    </div>
                  </div>
                )}

                {messages.map((message) => (
                  <div
                    key={message.id}
                    className={`flex gap-3 ${message.role === 'user' ? 'justify-end' : 'justify-start'}`}
                  >
                    {message.role === 'assistant' && (
                      <Avatar className="w-8 h-8">
                        <AvatarFallback className="bg-primary/10 text-primary">
                          <Brain className="w-4 h-4" />
                        </AvatarFallback>
                      </Avatar>
                    )}

                    <div
                      className={`max-w-[70%] rounded-lg px-4 py-2 ${
                        message.role === 'user'
                          ? 'bg-primary text-primary-foreground'
                          : 'bg-muted'
                      }`}
                    >
                      <div className="whitespace-pre-wrap">
                        {formatMessageContent(message.content)}
                      </div>
                      <div className="flex items-center justify-between mt-2">
                        <p className="text-xs opacity-70">
                          {message.timestamp.toLocaleTimeString()}
                        </p>
                        {/* TTS Button - only for assistant messages and when auto-TTS is disabled */}
                        {message.role === 'assistant' && isTTSAvailable() && !isTtsEnabled && (
                          <Button
                            variant="ghost"
                            size="sm"
                            onClick={() => {
                              if (!OpenAITTS.isPlaying()) {
                                speakTextBySentences(message.content, message.id);
                              }
                            }}
                            className={`h-6 w-6 p-0 ${
                              speakingMessageId === message.id
                                ? 'text-red-500 hover:text-red-600'
                                : 'text-muted-foreground hover:text-foreground'
                            }`}
                            title={speakingMessageId === message.id ? '–û—Å—Ç–∞–Ω–æ–≤–∏—Ç—å –æ–∑–≤—É—á–∏–≤–∞–Ω–∏–µ' : '–û–∑–≤—É—á–∏—Ç—å —Å–æ–æ–±—â–µ–Ω–∏–µ'}
                          >
                            {speakingMessageId === message.id ? (
                              <VolumeX className="h-3 w-3" />
                            ) : (
                              <Volume2 className="h-3 w-3" />
                            )}
                          </Button>
                        )}

                        {/* Auto-TTS indicator - show when auto-TTS is enabled */}
                        {message.role === 'assistant' && isTtsEnabled && (
                          <div className="flex items-center text-green-600" title="–ê–≤—Ç–æ-–æ–∑–≤—É—á–∏–≤–∞–Ω–∏–µ –≤–∫–ª—é—á–µ–Ω–æ">
                            <Volume2 className="h-3 w-3" />
                          </div>
                        )}
                      </div>
                    </div>

                    {message.role === 'user' && (
                      <Avatar className="w-8 h-8">
                        <AvatarFallback className="bg-accent text-accent-foreground">
                          <User className="w-4 h-4" />
                        </AvatarFallback>
                      </Avatar>
                    )}
                  </div>
                ))}

                {isLoading && (
                  <div className="flex gap-3 justify-start">
                    <Avatar className="w-8 h-8">
                      <AvatarFallback className="bg-primary/10 text-primary">
                        <Brain className="w-4 h-4" />
                      </AvatarFallback>
                    </Avatar>
                    <div className="bg-muted rounded-lg px-4 py-2">
                      <div className="flex items-center gap-2">
                        <div className="flex gap-1">
                          <div className="w-2 h-2 bg-primary/50 rounded-full animate-bounce" />
                          <div className="w-2 h-2 bg-primary/50 rounded-full animate-bounce" style={{ animationDelay: '0.1s' }} />
                          <div className="w-2 h-2 bg-primary/50 rounded-full animate-bounce" style={{ animationDelay: '0.2s' }} />
                        </div>
                        <span className="text-sm text-muted-foreground">–£—á–∏—Ç–µ–ª—å –¥—É–º–∞–µ—Ç...</span>
                      </div>
                    </div>
                  </div>
                )}
              </div>
            </ScrollArea>

            {/* Camera Interface */}
            {isCameraActive && (
              <div className="mb-4 p-4 border rounded-lg bg-muted/30">
                <div className="relative">
                  <video
                    ref={videoRef}
                    autoPlay
                    playsInline
                    muted
                    className="w-full max-h-64 bg-black rounded"
                  />
                  <canvas ref={canvasRef} className="hidden" />
                  <div className="absolute bottom-2 left-1/2 transform -translate-x-1/2 flex gap-2">
                    <Button
                      size="sm"
                      onClick={capturePhoto}
                      className="bg-primary hover:bg-primary/90"
                    >
                      üì∏ –°—Ñ–æ—Ç–æ–≥—Ä–∞—Ñ–∏—Ä–æ–≤–∞—Ç—å
                    </Button>
                    <Button
                      size="sm"
                      variant="outline"
                      onClick={stopCamera}
                      className="bg-black/50 text-white border-white/30 hover:bg-black/70"
                    >
                      ‚ùå –û—Ç–º–µ–Ω–∞
                    </Button>
                  </div>
                </div>
              </div>
            )}

            {/* Captured Image Preview */}
            {capturedImage && (
              <div className="mb-4 p-4 border rounded-lg bg-muted/30">
                <div className="text-center">
                  <img
                    src={capturedImage}
                    alt="–°—Ñ–æ—Ç–æ–≥—Ä–∞—Ñ–∏—Ä–æ–≤–∞–Ω–Ω–∞—è –∑–∞–¥–∞—á–∞"
                    className="max-h-64 mx-auto rounded"
                  />
                  <div className="mt-3 flex gap-2 justify-center">
                    <Button
                      size="sm"
                      onClick={sendCapturedPhoto}
                      disabled={isLoading}
                    >
                      {isLoading ? "–ê–Ω–∞–ª–∏–∑–∏—Ä—É—é..." : "üß† –†–µ—à–∏—Ç—å –∑–∞–¥–∞—á—É"}
                    </Button>
                    <Button
                      size="sm"
                      variant="outline"
                      onClick={retakePhoto}
                      disabled={isLoading}
                    >
                      üîÑ –ü–µ—Ä–µ—Å–Ω—è—Ç—å
                    </Button>
                  </div>
                </div>
              </div>
            )}

            {/* Voice Chat Status */}
            {isVoiceChatActive && (
              <div className="px-4 py-2 border-t bg-blue-50 dark:bg-blue-950/20">
                <div className="flex items-center gap-2 text-sm text-blue-700 dark:text-blue-300">
                  {isListening ? (
                    <>
                      <Mic className="w-4 h-4 animate-pulse text-green-600" />
                      <span>üé§ –°–ª—É—à–∞—é - –≥–æ–≤–æ—Ä–∏—Ç–µ –≤–∞—à –≤–æ–ø—Ä–æ—Å...</span>
                    </>
                  ) : isGeneratingTTS ? (
                    <>
                      <Brain className="w-4 h-4 animate-pulse text-purple-600" />
                      <span>üéµ –ü–æ–¥–≥–æ—Ç–æ–≤–∫–∞ –æ—Ç–≤–µ—Ç–∞...</span>
                    </>
                  ) : OpenAITTS.isPlaying() ? (
                    <>
                      <Volume2 className="w-4 h-4 animate-pulse text-blue-600" />
                      <span>üîä –û—Ç–≤–µ—á–∞—é –≥–æ–ª–æ—Å–æ–º...</span>
                    </>
                  ) : (
                    <>
                      <Brain className="w-4 h-4 animate-pulse text-orange-600" />
                      <span>ü§î –û–±—Ä–∞–±–∞—Ç—ã–≤–∞—é –æ—Ç–≤–µ—Ç...</span>
                    </>
                  )}
                </div>
                {ttsInterrupted && (
                  <div className="mt-2 flex items-center gap-2 text-sm text-red-600 dark:text-red-400">
                    <span>‚ö° TTS –ø—Ä–µ—Ä–≤–∞–Ω —Ä–µ—á—å—é –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—è</span>
                  </div>
                )}
              </div>
            )}

            {/* Uploaded Files Display */}
            {uploadedFiles.length > 0 && (
              <div className="px-4 py-2 border-t bg-muted/30">
                <div className="flex flex-wrap gap-2">
                  {uploadedFiles.map((file, index) => (
                    <div
                      key={index}
                      className="flex items-center gap-2 bg-background rounded-lg px-3 py-2 border text-sm"
                    >
                      {getFileIcon(file)}
                      <span className="truncate max-w-32">{file.name}</span>
                      <Button
                        variant="ghost"
                        size="sm"
                        onClick={() => removeFile(index)}
                        className="h-4 w-4 p-0 hover:bg-destructive/20"
                      >
                        <X className="w-3 h-3" />
                      </Button>
                    </div>
                  ))}
                </div>
              </div>
            )}

            {/* Input Area */}
            <div className="flex flex-wrap gap-2 pt-4 border-t">
              <input
                ref={fileInputRef}
                type="file"
                accept="image/*,.pdf,.doc,.docx"
                multiple
                onChange={handleFileUpload}
                className="hidden"
              />
              <Button
                variant="outline"
                size="icon"
                onClick={() => fileInputRef.current?.click()}
                disabled={isLoading}
                title="–ó–∞–≥—Ä—É–∑–∏—Ç—å —Ñ–∞–π–ª (–∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–µ, PDF, DOCX)"
              >
                <Upload className="w-4 h-4" />
              </Button>

              {/* Camera Button */}
              <Button
                variant={isCameraActive ? "destructive" : "outline"}
                size="icon"
                onClick={isCameraActive ? stopCamera : startCamera}
                disabled={isLoading}
                title={isCameraActive ? "–ó–∞–∫—Ä—ã—Ç—å –∫–∞–º–µ—Ä—É" : "–°—Ñ–æ—Ç–æ–≥—Ä–∞—Ñ–∏—Ä–æ–≤–∞—Ç—å –∑–∞–¥–∞—á—É"}
                className={isCameraActive ? "animate-pulse" : ""}
              >
                <Camera className="w-4 h-4" />
              </Button>

                  <Input
                    ref={inputRef}
                    value={inputMessage}
                    onChange={(e) => setInputMessage(e.target.value)}
                    onKeyPress={handleKeyPress}
                    placeholder="–ó–∞–¥–∞–π—Ç–µ –≤–æ–ø—Ä–æ—Å –ø–æ –ª—é–±–æ–π —É—á–µ–±–Ω–æ–π —Ç–µ–º–µ..."
                    disabled={isLoading || isProcessingFile}
                    className="flex-1"
                  />


                  {/* Voice Chat Button */}
                  <Button
                    variant={isVoiceChatActive ? "default" : "outline"}
                    size="icon"
                    onClick={startVoiceChat}
                    disabled={!isTTSAvailable()}
                    title={isVoiceChatActive ? "–û—Å—Ç–∞–Ω–æ–≤–∏—Ç—å –≥–æ–ª–æ—Å–æ–≤–æ–µ –æ–±—â–µ–Ω–∏–µ" : `–ù–∞—á–∞—Ç—å –≥–æ–ª–æ—Å–æ–≤–æ–µ –æ–±—â–µ–Ω–∏–µ —Å AI –£—á–∏—Ç–µ–ª–µ–º${!('webkitSpeechRecognition' in window && 'SpeechRecognition' in window) ? ' (–ë—Ä–∞—É–∑–µ—Ä –º–æ–∂–µ—Ç –Ω–µ –ø–æ–¥–¥–µ—Ä–∂–∏–≤–∞—Ç—å —Ä–∞—Å–ø–æ–∑–Ω–∞–≤–∞–Ω–∏–µ —Ä–µ—á–∏)' : ''}`}
                    className={isVoiceChatActive ? "bg-red-600 hover:bg-red-700 animate-pulse" : ""}
                  >
                    {isVoiceChatActive ? (
                      isListening ? <Mic className="w-4 h-4 animate-pulse text-white" /> : <MicOff className="w-4 h-4" />
                    ) : (
                      <MessageCircle className="w-4 h-4" />
                    )}
                  </Button>

                  <Button
                    variant="outline"
                    size="icon"
                    onClick={clearMemory}
                    disabled={messages.length <= 1} // Disable if only system message
                    title="–û—á–∏—Å—Ç–∏—Ç—å –ø–∞–º—è—Ç—å —É—á–∏—Ç–µ–ª—è"
                  >
                    <Trash2 className="w-4 h-4" />
                  </Button>

                  <Button
                    onClick={sendMessage}
                    disabled={(!inputMessage.trim() && uploadedFiles.length === 0) || isLoading || isProcessingFile}
                    size="icon"
                  >
                {isProcessingFile ? (
                  <div className="w-4 h-4 border-2 border-primary border-t-transparent rounded-full animate-spin" />
                ) : (
                  <Send className="w-4 h-4" />
                )}
              </Button>
            </div>
          </CardContent>
        </Card>
      </div>
    </div>
  );
};

export default Chat;
