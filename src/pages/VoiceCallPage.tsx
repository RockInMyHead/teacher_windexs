import React, { useState, useRef, useEffect } from 'react';
import { useNavigate } from 'react-router-dom';
import { Button } from '@/components/ui/button';
import { Card, CardContent, CardHeader, CardTitle } from '@/components/ui/card';
import { ArrowLeft, Mic, Loader2, MicOff, PhoneOff } from 'lucide-react';
import Header from '@/components/Header';

interface Message {
  role: 'user' | 'assistant';
  content: string;
  timestamp: Date;
}

const VoiceCallPage: React.FC = () => {
  const navigate = useNavigate();

  // State
  const [isListening, setIsListening] = useState(false);
  const [isProcessing, setIsProcessing] = useState(false);
  const [isSpeaking, setIsSpeaking] = useState(false);
  const [isMuted, setIsMuted] = useState(false);
  const [messages, setMessages] = useState<Message[]>([]);
  const [error, setError] = useState<string | null>(null);
  const [speechTheses, setSpeechTheses] = useState<string[]>([]);
  
  // Use ref for lesson context to avoid closure issues
  const lessonContextRef = useRef<{
    title: string;
    topic: string;
    description: string;
  } | null>(null);

  // Refs
  const mediaRecorderRef = useRef<MediaRecorder | null>(null);
  const audioChunksRef = useRef<Blob[]>([]);
  const audioStreamRef = useRef<MediaStream | null>(null);
  const audioContextRef = useRef<AudioContext | null>(null);
  const analyserRef = useRef<AnalyserNode | null>(null);
  const animationFrameRef = useRef<number | null>(null);
  const speechFramesRef = useRef<number>(0);
  const silenceFramesRef = useRef<number>(0);
  const silenceAfterSpeechRef = useRef<number>(0);
  const speechDetectedRef = useRef<boolean>(false);
  const processingTypeRef = useRef<'speech' | 'silence' | null>(null);
  const isActiveRef = useRef<boolean>(false);
  const videoRef = useRef<HTMLVideoElement | null>(null);

  // Voice detection parameters
  const CALIBRATION_FRAMES = 30; // ~1.5 seconds to measure background noise (first time)
  const QUICK_CALIBRATION_FRAMES = 10; // ~0.5 seconds for quick recalibration after resume
  const REQUIRED_SPEECH_FRAMES = 8; // ~0.4 seconds of speech to mark as started (even faster response)
  const SILENCE_AFTER_SPEECH_FRAMES = 40; // ~2 seconds of silence after speech to stop
  const REQUIRED_SILENCE_FRAMES = 200; // ~10 seconds of total silence for follow-up (increased to avoid false triggers)
  
  // Dynamic noise detection
  const noiseFloorRef = useRef<number>(0);
  const isCalibrationDoneRef = useRef<boolean>(false);
  const calibrationSamplesRef = useRef<number[]>([]);
  const isQuickCalibrationRef = useRef<boolean>(false); // Quick recalibration after resume

  // Toggle microphone mute/unmute
  const toggleMute = () => {
    if (isMuted) {
      // Unmute - resume listening
      setIsMuted(false);
      console.log('üé§ Microphone unmuted');
      if (!isListening && !isProcessing) {
        startListening();
      }
    } else {
      // Mute - stop listening
      setIsMuted(true);
      console.log('üîá Microphone muted');
      stopRecording();
    }
  };

  // End lesson and navigate back
  const endLesson = () => {
    console.log('üìû Ending lesson');
    stopRecording();
    cleanup();
    setSpeechTheses([]);
    navigate(-1);
  };

  // Cleanup function
  const cleanup = () => {
    console.log('üßπ Cleanup started');
    
    if (animationFrameRef.current) {
      cancelAnimationFrame(animationFrameRef.current);
      animationFrameRef.current = null;
    }
    
    if (audioContextRef.current && audioContextRef.current.state !== 'closed') {
      audioContextRef.current.close().catch(console.error);
      audioContextRef.current = null;
    }
    
      if (mediaRecorderRef.current && mediaRecorderRef.current.state !== 'inactive') {
      try {
        mediaRecorderRef.current.stop();
      } catch (e) {
        console.warn('MediaRecorder stop error:', e);
      }
    }
    mediaRecorderRef.current = null;
    
    if (audioStreamRef.current) {
      audioStreamRef.current.getTracks().forEach(track => track.stop());
      audioStreamRef.current = null;
    }
    
    analyserRef.current = null;
    audioChunksRef.current = [];
    speechFramesRef.current = 0;
    silenceFramesRef.current = 0;
    silenceAfterSpeechRef.current = 0;
    speechDetectedRef.current = false;
    processingTypeRef.current = null;
    isActiveRef.current = false;
    
    console.log('‚úÖ Cleanup complete');
  };

  // Start listening
  const startListening = async () => {
    if (isActiveRef.current) {
      console.log('‚ö†Ô∏è Already active, skipping start');
      return;
    }

    try {
      console.log('üé§ Starting listening...');
      cleanup();
      
      isActiveRef.current = true;
      setIsListening(true);
      setError(null);
      
      // Reset detection state
      speechFramesRef.current = 0;
      silenceFramesRef.current = 0;
      silenceAfterSpeechRef.current = 0;
      speechDetectedRef.current = false;
      processingTypeRef.current = null;
      
      // Reset noise calibration (full calibration)
      isCalibrationDoneRef.current = false;
      calibrationSamplesRef.current = [];
      noiseFloorRef.current = 0;
      isQuickCalibrationRef.current = false; // Full calibration on start

      // Get microphone access
      const stream = await navigator.mediaDevices.getUserMedia({ 
        audio: {
          echoCancellation: true,
          noiseSuppression: true,
          autoGainControl: true
        } 
      });
      audioStreamRef.current = stream;
      console.log('‚úÖ Microphone access granted');

      // Setup MediaRecorder
      const mimeType = MediaRecorder.isTypeSupported('audio/webm;codecs=opus') 
        ? 'audio/webm;codecs=opus' 
        : 'audio/webm';
      
      const mediaRecorder = new MediaRecorder(stream, { mimeType });
      mediaRecorderRef.current = mediaRecorder;
      audioChunksRef.current = [];

      mediaRecorder.ondataavailable = (event) => {
        if (event.data.size > 0) {
          audioChunksRef.current.push(event.data);
          console.log('üìº Audio chunk received, size:', event.data.size, 'total chunks:', audioChunksRef.current.length);
        }
      };

      mediaRecorder.onstop = async () => {
        console.log('üé¨ ONSTOP TRIGGERED!');
        const processingType = processingTypeRef.current;
        console.log('üé§ Recording stopped, type:', processingType, 'audioChunks:', audioChunksRef.current.length);
        
        if (!processingType) {
          console.warn('‚ö†Ô∏è No processing type set, restarting...');
          restartListening();
          return;
        }

        if (audioChunksRef.current.length === 0) {
          console.warn('‚ö†Ô∏è No audio data, restarting...');
          restartListening();
          return;
        }

        const audioBlob = new Blob(audioChunksRef.current, { type: mimeType });
        console.log('üì¶ Audio blob created - size:', audioBlob.size, 'bytes, type:', audioBlob.type);

        if (audioBlob.size < 5000) {
          console.warn('‚ö†Ô∏è Audio too small (', audioBlob.size, 'bytes), skipping and restarting...');
          restartListening();
          return;
        }

        // Process based on type
        console.log('‚úÖ Processing audio, type:', processingType);
        if (processingType === 'speech') {
          await handleSpeech(audioBlob);
        } else if (processingType === 'silence') {
          // Silence detected - just restart listening without generating follow-up
          console.log('üîÑ Silence detected, restarting listening...');
          restartListening();
        }
      };

      // Start recording
      mediaRecorder.start();
      console.log('üéôÔ∏è Recording started');

      // Setup audio analysis
      setupAudioAnalysis(stream);

    } catch (error) {
      console.error('‚ùå Start listening error:', error);
      setError('–û—à–∏–±–∫–∞ –¥–æ—Å—Ç—É–ø–∞ –∫ –º–∏–∫—Ä–æ—Ñ–æ–Ω—É');
      isActiveRef.current = false;
      setIsListening(false);
    }
  };

  // Setup audio analysis
  const setupAudioAnalysis = (stream: MediaStream) => {
    try {
      const audioContext = new (window.AudioContext || (window as any).webkitAudioContext)();
      audioContextRef.current = audioContext;

      const analyser = audioContext.createAnalyser();
      analyser.fftSize = 256;
      analyser.smoothingTimeConstant = 0.3;

      const source = audioContext.createMediaStreamSource(stream);
      source.connect(analyser);
      analyserRef.current = analyser;

      console.log('üéß Audio analysis ready');

      // Start detection loop
      detectAudio();
    } catch (error) {
      console.error('‚ùå Audio analysis setup error:', error);
    }
  };

  // Detect audio levels with adaptive noise floor
  const detectAudio = () => {
    if (!isActiveRef.current || !analyserRef.current) {
      console.log('üõë Detection stopped');
      return;
    }

    const analyser = analyserRef.current;
    const bufferLength = analyser.frequencyBinCount;
    const dataArray = new Uint8Array(bufferLength);
    
    analyser.getByteFrequencyData(dataArray);
    
    // Calculate average and max energy in voice frequency range (roughly bins 10-100 for typical sample rates)
    // Human voice is typically 85-255 Hz (low) to 3400 Hz (high)
    // We focus on bins that represent ~300-3000 Hz
    const voiceStartBin = Math.floor(bufferLength * 0.05); // ~5% of spectrum
    const voiceEndBin = Math.floor(bufferLength * 0.4); // ~40% of spectrum
    
    let sum = 0;
    let max = 0;
    let count = 0;
    
    for (let i = voiceStartBin; i < voiceEndBin && i < bufferLength; i++) {
      sum += dataArray[i];
      if (dataArray[i] > max) max = dataArray[i];
      count++;
    }
    
    const average = count > 0 ? sum / count : 0;
    
    // Calibration phase: measure background noise
    if (!isCalibrationDoneRef.current) {
      // Only add non-zero samples to calibration
      if (average > 1) {
        calibrationSamplesRef.current.push(average);
      }
      
      // Use quick calibration (0.5s) for resume, full calibration (1.5s) for initial start
      const requiredFrames = isQuickCalibrationRef.current ? QUICK_CALIBRATION_FRAMES : CALIBRATION_FRAMES;
      
      if (calibrationSamplesRef.current.length >= requiredFrames) {
        // Calculate noise floor as average of calibration samples
        const noiseSum = calibrationSamplesRef.current.reduce((a, b) => a + b, 0);
        const measuredNoiseFloor = noiseSum / calibrationSamplesRef.current.length;
        
        // Set minimum noise floor to avoid zero threshold (lowered to 5 for very quiet environments)
        noiseFloorRef.current = Math.max(measuredNoiseFloor, 5);
        
        isCalibrationDoneRef.current = true;
        const calibType = isQuickCalibrationRef.current ? 'Quick' : 'Full';
        console.log(`üéöÔ∏è ${calibType} calibration: measured=${measuredNoiseFloor.toFixed(2)}, actual=${noiseFloorRef.current.toFixed(2)}, threshold=${(noiseFloorRef.current * 2.0).toFixed(2)}`);
      } else {
        // Still calibrating, continue
        animationFrameRef.current = requestAnimationFrame(detectAudio);
        return;
      }
    }
    
    // Dynamic speech threshold: noise floor * 1.4 (—Ä–µ—á—å –¥–æ–ª–∂–Ω–∞ –±—ã—Ç—å –≥—Ä–æ–º—á–µ —Ñ–æ–Ω–∞)
    const MIN_THRESHOLD = 15; // –ú–∏–Ω–∏–º–∞–ª—å–Ω—ã–π –∞–±—Å–æ–ª—é—Ç–Ω—ã–π –ø–æ—Ä–æ–≥ (lowered for very quiet speech)
    const dynamicThreshold = Math.max(noiseFloorRef.current * 1.4, MIN_THRESHOLD);
    
    // Periodic logging to debug detection issues (every 100 frames = ~5 seconds)
    if (speechFramesRef.current === 0 && silenceFramesRef.current % 100 === 0 && silenceFramesRef.current > 0) {
      console.log(`üëÇ Listening... avg=${average.toFixed(1)}, max=${max}, threshold=${dynamicThreshold.toFixed(1)} (speak louder if not detecting)`);
    }
    
    // –ü–æ—Å–ª–µ –Ω–∞—á–∞–ª–∞ —Ä–µ—á–∏ –∏—Å–ø–æ–ª—å–∑—É–µ–º –±–æ–ª–µ–µ –Ω–∏–∑–∫–∏–π –ø–æ—Ä–æ–≥ –¥–ª—è –¥–µ—Ç–µ–∫—Ü–∏–∏ —Ç–∏—à–∏–Ω—ã
    const silenceThreshold = speechDetectedRef.current
      ? Math.max(noiseFloorRef.current * 1.2, 8) // –ë–æ–ª–µ–µ –Ω–∏–∑–∫–∏–π –ø–æ—Ä–æ–≥ –ø–æ—Å–ª–µ —Ä–µ—á–∏ (much lower)
      : dynamicThreshold; // –ò—Å—Ö–æ–¥–Ω—ã–π –ø–æ—Ä–æ–≥ –¥–ª—è –Ω–∞—á–∞–ª–∞ —Ä–µ—á–∏
    
    // –†–µ—á—å –¥–µ—Ç–µ–∫—Ç–∏—Ä—É–µ—Ç—Å—è –µ—Å–ª–∏:
    // 1. –°—Ä–µ–¥–Ω—è—è —ç–Ω–µ—Ä–≥–∏—è –ø—Ä–µ–≤—ã—à–∞–µ—Ç –ø–æ—Ä–æ–≥
    // 2. –ò –µ—Å—Ç—å —è—Ä–∫–æ –≤—ã—Ä–∞–∂–µ–Ω–Ω—ã–π –ø–∏–∫ (max > avg * 1.3)
    const isSpeech = speechDetectedRef.current 
      ? average > silenceThreshold // –ü–æ—Å–ª–µ –Ω–∞—á–∞–ª–∞ —Ä–µ—á–∏ –∏—Å–ø–æ–ª—å–∑—É–µ–º –Ω–∏–∑–∫–∏–π –ø–æ—Ä–æ–≥
      : average > dynamicThreshold && max > average * 1.3; // –î–ª—è –Ω–∞—á–∞–ª–∞ —Ä–µ—á–∏ —Å—Ç—Ä–æ–≥–∏–µ –∫—Ä–∏—Ç–µ—Ä–∏–∏
    
    if (isSpeech) {
      // Speech detected
      speechFramesRef.current++;
      silenceAfterSpeechRef.current = 0;

      // Mark that speech was detected
      if (speechFramesRef.current >= REQUIRED_SPEECH_FRAMES && !speechDetectedRef.current) {
        console.log(`üé§ SPEECH STARTED! avg=${average.toFixed(1)}, max=${max}, threshold=${dynamicThreshold.toFixed(1)}, silence_threshold=${silenceThreshold.toFixed(1)}`);
        speechDetectedRef.current = true;
      }
      
      // Log every 50 frames to monitor
      if (speechDetectedRef.current && speechFramesRef.current % 50 === 0) {
        console.log(`üó£Ô∏è Speaking... frames=${speechFramesRef.current}, avg=${average.toFixed(1)}, max=${max}, silence_threshold=${silenceThreshold.toFixed(1)}`);
      }
    } else {
      // Silence detected
      if (speechDetectedRef.current) {
        // We detected speech earlier, now counting silence after it
        silenceAfterSpeechRef.current++;
        
        if (silenceAfterSpeechRef.current === 1) {
          console.log(`ü§´ Silence detected: avg=${average.toFixed(1)}, silence_threshold=${silenceThreshold.toFixed(1)}`);
        }
        
        if (silenceAfterSpeechRef.current % 20 === 0) {
          console.log(`ü§´ Silence progress: ${silenceAfterSpeechRef.current}/${SILENCE_AFTER_SPEECH_FRAMES}, avg=${average.toFixed(1)}`);
        }
        
        if (silenceAfterSpeechRef.current >= SILENCE_AFTER_SPEECH_FRAMES) {
          console.log(`‚úÖ SPEECH ENDED after ${silenceAfterSpeechRef.current} frames of silence`);
          processingTypeRef.current = 'speech';
          stopRecording();
          return;
        }
      } else {
        // No speech yet, just reset speech counter and continue listening
        silenceFramesRef.current++;
        speechFramesRef.current = 0;
        
        // Don't generate follow-up questions on silence - just keep listening
        // User will speak when ready
      }
    }

    // Continue detection
    animationFrameRef.current = requestAnimationFrame(detectAudio);
  };

  // Stop recording
  const stopRecording = () => {
    console.log('‚èπÔ∏è Stop recording called, processingType:', processingTypeRef.current);

    // Stop animation frame
    if (animationFrameRef.current) {
      cancelAnimationFrame(animationFrameRef.current);
      animationFrameRef.current = null;
      console.log('‚úÖ Animation frame cancelled');
    }

    // Stop media recorder
    if (mediaRecorderRef.current) {
      console.log('üéôÔ∏è MediaRecorder state:', mediaRecorderRef.current.state);
      if (mediaRecorderRef.current.state === 'recording') {
        console.log('‚èπÔ∏è Stopping MediaRecorder...');
      mediaRecorderRef.current.stop();
      } else {
        console.warn('‚ö†Ô∏è MediaRecorder not in recording state:', mediaRecorderRef.current.state);
      }
    } else {
      console.warn('‚ö†Ô∏è No MediaRecorder reference');
    }

    // Close audio context
    if (audioContextRef.current && audioContextRef.current.state !== 'closed') {
      audioContextRef.current.close();
      audioContextRef.current = null;
      console.log('‚úÖ AudioContext closed');
    }

    isActiveRef.current = false;
    setIsListening(false);
    console.log('‚úÖ Stop recording complete');
  };

  // Handle user speech
  const handleSpeech = async (audioBlob: Blob) => {
    // Prevent concurrent processing
    if (isProcessing) {
      console.warn('‚ö†Ô∏è Already processing speech, skipping...');
      return;
    }

    try {
      console.log('üîä Processing speech...');
      setIsProcessing(true);

      // Transcribe
      const transcription = await transcribeAudio(audioBlob);
      console.log('üìù Transcription:', transcription);

      if (!transcription || transcription.trim().length < 2) {
        console.warn('‚ö†Ô∏è Transcription too short');
        setIsProcessing(false);
        setIsSpeaking(false);
        resumeListening();
        return;
      }

      // Check for emoji or weird characters (Whisper hallucinations)
      const hasOnlyEmoji = /^[\p{Emoji}\s]+$/u.test(transcription.trim());
      const hasWeirdChars = /[^\w\s–∞-—è—ë\-.,!?;:()"\s]/gi.test(transcription.trim());

      if (hasOnlyEmoji || hasWeirdChars) {
        console.warn('‚ö†Ô∏è Transcription contains only emoji or weird characters:', transcription);
        setIsProcessing(false);
        setIsSpeaking(false);
        resumeListening();
        return;
      }
      
      // Add user message
      setMessages(prev => [...prev, {
        role: 'user',
        content: transcription,
        timestamp: new Date()
      }]);

      // Get LLM response
      const response = await getLLMResponse(transcription);
      console.log('ü§ñ LLM:', response);
      
      // Extract theses from response
      const theses = extractTheses(response);
      setSpeechTheses(theses);
      
      // Remove "–ö–ª—é—á–µ–≤—ã–µ —Ç–µ–∑–∏—Å—ã" section from TTS (remove everything after "–ö–ª—é—á–µ–≤—ã–µ —Ç–µ–∑–∏—Å—ã")
      let textForTTS = response.replace(/–ö–ª—é—á–µ–≤—ã–µ —Ç–µ–∑–∏—Å—ã[\s\S]*$/i, '').trim();
      
      // Add assistant message
      setMessages(prev => [...prev, {
        role: 'assistant',
        content: response,
        timestamp: new Date()
      }]);

      setIsProcessing(false);

      // Speak response (without theses)
      setIsSpeaking(true);
      await speakText(textForTTS);
      setIsSpeaking(false);

      // Resume listening immediately (no delay needed)
      resumeListening();
      
    } catch (error) {
      console.error('‚ùå Handle speech error:', error);
      setIsProcessing(false);
      setIsSpeaking(false);
      resumeListening();
    }
  };

  // Handle silence
  const handleSilence = async () => {
    try {
      console.log('ü§´ Processing silence...');
      setIsProcessing(true);

      const message = "–ï—Å—Ç—å –≤–æ–ø—Ä–æ—Å—ã? –Ø –≥–æ—Ç–æ–≤–∞ –ø–æ–º–æ—á—å!";
      
      setMessages(prev => [...prev, {
        role: 'assistant',
        content: message,
        timestamp: new Date()
      }]);

      setIsProcessing(false);
      setIsSpeaking(true);
      await speakText(message);
      setIsSpeaking(false);

      // Add delay before restarting to prevent echo
      console.log('‚è∏Ô∏è Waiting 2 seconds before restart to prevent echo...');
      await new Promise(resolve => setTimeout(resolve, 2000));

      restartListening();

    } catch (error) {
      console.error('‚ùå Handle silence error:', error);
      setIsProcessing(false);
      setIsSpeaking(false);
      restartListening();
    }
  };

  // Resume listening immediately (without recalibration)
  const resumeListening = async () => {
    if (isActiveRef.current) {
      console.log('‚ö†Ô∏è Already active, skipping resume');
      return;
    }

    try {
      console.log('‚ö° Resuming listening immediately...');
      
      // Reset detection state
      speechFramesRef.current = 0;
      silenceFramesRef.current = 0;
      silenceAfterSpeechRef.current = 0;
      speechDetectedRef.current = false;
      processingTypeRef.current = null;
      
      // Quick recalibration (0.5s) to adapt to current noise level
      isCalibrationDoneRef.current = false;
      calibrationSamplesRef.current = [];
      isQuickCalibrationRef.current = true;
      
      isActiveRef.current = true;
      setIsListening(true);
      setError(null);

      // Reuse existing stream or get new one
      let stream = audioStreamRef.current;
      if (!stream || !stream.active) {
        stream = await navigator.mediaDevices.getUserMedia({ 
          audio: {
            echoCancellation: true,
            noiseSuppression: true,
            autoGainControl: true
          } 
        });
        audioStreamRef.current = stream;
        console.log('‚úÖ New microphone stream');
      } else {
        console.log('‚ôªÔ∏è Reusing existing stream');
      }

      // Setup new MediaRecorder
      const mimeType = MediaRecorder.isTypeSupported('audio/webm;codecs=opus') 
        ? 'audio/webm;codecs=opus' 
        : 'audio/webm';
      
      const mediaRecorder = new MediaRecorder(stream, { mimeType });
      mediaRecorderRef.current = mediaRecorder;
      audioChunksRef.current = [];

      mediaRecorder.ondataavailable = (event) => {
        if (event.data.size > 0) {
          audioChunksRef.current.push(event.data);
        }
      };

      mediaRecorder.onstop = async () => {
        const processingType = processingTypeRef.current;
        
        if (!processingType) {
          resumeListening();
          return;
        }

        if (audioChunksRef.current.length === 0) {
          resumeListening();
          return;
        }

        const audioBlob = new Blob(audioChunksRef.current, { type: mimeType });

        if (audioBlob.size < 5000) {
          resumeListening();
          return;
        }

        if (processingType === 'speech') {
          await handleSpeech(audioBlob);
        } else if (processingType === 'silence') {
          resumeListening();
        }
      };

      mediaRecorder.start();
      console.log('üéôÔ∏è Recording resumed');

      // Setup audio analysis (reuse context if possible)
      if (!audioContextRef.current || audioContextRef.current.state === 'closed') {
        setupAudioAnalysis(stream);
      } else {
        // Reconnect analyser to stream
        const source = audioContextRef.current.createMediaStreamSource(stream);
        if (analyserRef.current) {
          source.connect(analyserRef.current);
        }
        console.log('‚ôªÔ∏è Reusing AudioContext');
        
        // Start detection immediately
        detectAudio();
      }

    } catch (error) {
      console.error('‚ùå Resume listening error:', error);
      setError('–û—à–∏–±–∫–∞ –¥–æ—Å—Ç—É–ø–∞ –∫ –º–∏–∫—Ä–æ—Ñ–æ–Ω—É');
      isActiveRef.current = false;
      setIsListening(false);
    }
  };

  // Restart listening (full reset with recalibration)
  const restartListening = () => {
    console.log('üîÑ Restarting listening...');

    // Reset all detection state
    speechFramesRef.current = 0;
    silenceFramesRef.current = 0;
    silenceAfterSpeechRef.current = 0;
    speechDetectedRef.current = false;
    processingTypeRef.current = null;
    
    // Reset noise calibration (full calibration)
    isCalibrationDoneRef.current = false;
    calibrationSamplesRef.current = [];
    noiseFloorRef.current = 0;
    isQuickCalibrationRef.current = false; // Full calibration
    
    setTimeout(() => startListening(), 1500);
  };

  // Transcribe audio
  const transcribeAudio = async (audioBlob: Blob): Promise<string> => {
      const formData = new FormData();
      formData.append('file', audioBlob, 'audio.webm');
      formData.append('model', 'whisper-1');
      
      // Determine language based on lesson context
      const lessonContext = lessonContextRef.current;
      let language = 'ru'; // Default to Russian
      
      if (lessonContext) {
        const title = lessonContext.title.toLowerCase();
        const description = lessonContext.description?.toLowerCase() || '';
        
        // Check if it's an English lesson
        if (title.includes('english') || title.includes('–∞–Ω–≥–ª–∏–π—Å–∫–∏–π') || 
            title.includes('–∞–Ω–≥–ª.') || description.includes('english')) {
          language = 'en';
          console.log('üåç Detected English lesson, using language: en');
        } else if (title.includes('–∫–∏—Ç–∞–π—Å–∫–∏–π') || title.includes('chinese')) {
          language = 'zh';
          console.log('üåç Detected Chinese lesson, using language: zh');
        } else {
          console.log('üåç Using default language: ru');
        }
      }
      
      formData.append('language', language);

      const response = await fetch('/api/audio/transcriptions', {
        method: 'POST',
        body: formData
      });

      if (!response.ok) {
      throw new Error('Transcription failed');
    }

    const result = await response.json();
    return result.text || '';
  };

  // Extract key theses from LLM response
  const extractTheses = (response: string): string[] => {
    const theses: string[] = [];
    
    // Look for "–ö–ª—é—á–µ–≤—ã–µ —Ç–µ–∑–∏—Å—ã" section - capture until end of text
    const thesesMatch = response.match(/–ö–ª—é—á–µ–≤—ã–µ —Ç–µ–∑–∏—Å—ã[\s\S]*$/i);
    if (!thesesMatch) {
      console.log('‚ùå No "–ö–ª—é—á–µ–≤—ã–µ —Ç–µ–∑–∏—Å—ã" section found');
      return theses;
    }
    
    const thesesText = thesesMatch[0];
    console.log('üìã Theses section found, length:', thesesText.length);
    
    // Extract numbered items - support both digits (1., 2., 3.) and words (–æ–¥–∏–Ω., –¥–≤–∞., —Ç—Ä–∏.)
    const numberWords = ['–æ–¥–∏–Ω', '–¥–≤–∞', '—Ç—Ä–∏', '—á–µ—Ç—ã—Ä–µ', '–ø—è—Ç—å', '–ø–µ—Ä–≤—ã–π', '–≤—Ç–æ—Ä–æ–π', '—Ç—Ä–µ—Ç–∏–π'];
    
    // Pattern for digit numbering: "1. Text" or "1) Text"
    const digitPattern = /(?:^|\n)\s*(\d+)[.\)]\s*([^\n]+)/g;
    
    // Pattern for word numbering: "–æ–¥–∏–Ω. Text" or "–ü–µ—Ä–≤—ã–π. Text"
    const wordPattern = new RegExp(`(?:^|\\n)\\s*(${numberWords.join('|')})[.\\):]\\s*([^\\n]+)`, 'gi');
    
    let match;
    
    // Try digit pattern first
    while ((match = digitPattern.exec(thesesText)) !== null) {
      const cleanItem = match[2].trim();
      if (cleanItem && cleanItem.length > 0 && cleanItem.length < 150) {
        console.log('‚úÖ Found digit thesis:', cleanItem);
        theses.push(cleanItem);
      }
    }
    
    // If no digit pattern found, try word pattern
    if (theses.length === 0) {
      console.log('‚ö†Ô∏è No digit theses found, trying word pattern...');
      while ((match = wordPattern.exec(thesesText)) !== null) {
        const cleanItem = match[2].trim();
        if (cleanItem && cleanItem.length > 0 && cleanItem.length < 150) {
          console.log('‚úÖ Found word thesis:', cleanItem);
          theses.push(cleanItem);
        }
      }
    }
    
    console.log('üìå Total theses extracted:', theses.length);
    return theses.slice(0, 3); // Max 3 theses
  };

  // Get LLM response using GPT-5.1
  const getLLMResponse = async (userMessage: string): Promise<string> => {
    // Build lesson context if available (read from ref to avoid closure issues)
    const lessonContext = lessonContextRef.current;
    let lessonContextText = '';

    if (lessonContext) {
      lessonContextText = `

–¢–ï–ö–£–©–ò–ô –£–†–û–ö:
–ù–∞–∑–≤–∞–Ω–∏–µ: "${lessonContext.title}"
–¢–µ–º–∞: ${lessonContext.topic}
–°–æ–¥–µ—Ä–∂–∞–Ω–∏–µ —É—Ä–æ–∫–∞: ${lessonContext.description}`;
    } else {
      console.warn('‚ö†Ô∏è No lesson context available');
    }

    // Always use Russian prompt for Julia, regardless of lesson type
    const systemPrompt = `–¢—ã –Æ–ª–∏—è - –ø—Ä–æ—Ñ–µ—Å—Å–∏–æ–Ω–∞–ª—å–Ω—ã–π –ø–µ–¥–∞–≥–æ–≥ —Å 20-–ª–µ—Ç–Ω–∏–º —Å—Ç–∞–∂–µ–º. –í–µ–¥—ë—à—å —É—Ä–æ–∫ –ø–æ –≥–æ–ª–æ—Å–æ–≤–æ–π —Å–≤—è–∑–∏ –æ–¥–∏–Ω-–Ω–∞-–æ–¥–∏–Ω.${lessonContextText}

–¢–í–û–Ø –ú–ï–¢–û–î–ò–ö–ê:
1. –î–∞–≤–∞–π —Ç–æ–ª—å–∫–æ –û–î–ù–û –∑–∞–¥–∞–Ω–∏–µ –∑–∞ —Ä–∞–∑. –ù–µ –ø–µ—Ä–µ–≥—Ä—É–∂–∞–π —É—á–µ–Ω–∏–∫–∞.
2. –û–±—ä—è—Å–Ω—è–π "–Ω–∞ –ø–∞–ª—å—Ü–∞—Ö" - –ø—Ä–æ—Å—Ç–æ, –ø–æ–Ω—è—Ç–Ω–æ, —Å –∫–æ–Ω–∫—Ä–µ—Ç–Ω—ã–º–∏ –ø—Ä–∏–º–µ—Ä–∞–º–∏ –∏–∑ –∂–∏–∑–Ω–∏.
3. –ù–∞—á–∏–Ω–∞–π —Å —Å–∞–º–æ–≥–æ –ø—Ä–æ—Å—Ç–æ–≥–æ, –ø–æ—Å—Ç–µ–ø–µ–Ω–Ω–æ —É—Å–ª–æ–∂–Ω—è–π.
4. –ü–æ—Å–ª–µ –∫–∞–∂–¥–æ–≥–æ –æ—Ç–≤–µ—Ç–∞ —É—á–µ–Ω–∏–∫–∞ - –¥–∞–π –æ–±—Ä–∞—Ç–Ω—É—é —Å–≤—è–∑—å (–ø–æ—Ö–≤–∞–ª–∏ –∏–ª–∏ –º—è–≥–∫–æ –ø–æ–ø—Ä–∞–≤—å).
5. –ó–∞–¥–∞–≤–∞–π –≤–æ–ø—Ä–æ—Å—ã, —á—Ç–æ–±—ã –ø—Ä–æ–≤–µ—Ä–∏—Ç—å –ø–æ–Ω–∏–º–∞–Ω–∏–µ.
6. –ì–æ–≤–æ—Ä–∏ –∫—Ä–∞—Ç–∫–æ (2-3 –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∏—è) - —ç—Ç–æ –≥–æ–ª–æ—Å–æ–≤–æ–π —É—Ä–æ–∫, –Ω–µ –ª–µ–∫—Ü–∏—è.

–í–ê–ñ–ù–´–ï –ü–†–ê–í–ò–õ–ê:
- –í–°–ï –¶–ò–§–†–´ –ü–ò–®–ò –°–õ–û–í–ê–ú–ò: –≤–º–µ—Å—Ç–æ "1, 2, 3" –ø–∏—à–∏ "–æ–¥–∏–Ω, –¥–≤–∞, —Ç—Ä–∏"
- –í–º–µ—Å—Ç–æ "5 –º–∏–Ω—É—Ç" –ø–∏—à–∏ "–ø—è—Ç—å –º–∏–Ω—É—Ç"
- –í–º–µ—Å—Ç–æ "10 —Å–ª–æ–≤" –ø–∏—à–∏ "–¥–µ—Å—è—Ç—å —Å–ª–æ–≤"
- –≠—Ç–æ –≥–æ–ª–æ—Å–æ–≤–æ–π —É—Ä–æ–∫ - —á–∏—Å–ª–∞ –¥–æ–ª–∂–Ω—ã –±—ã—Ç—å –ø–æ–Ω—è—Ç–Ω—ã –ø—Ä–∏ –ø—Ä–æ–∏–∑–Ω–µ—Å–µ–Ω–∏–∏

–°–¢–†–£–ö–¢–£–†–ê –£–†–û–ö–ê:
- –ï—Å–ª–∏ —ç—Ç–æ –ø–µ—Ä–≤–æ–µ —Å–æ–æ–±—â–µ–Ω–∏–µ: –ø–æ–ø—Ä–∏–≤–µ—Ç—Å—Ç–≤—É–π, —Å–∫–∞–∂–∏ —Ç–µ–º—É —É—Ä–æ–∫–∞, –¥–∞–π –û–î–ù–û –ø—Ä–æ—Å—Ç–æ–µ –∑–∞–¥–∞–Ω–∏–µ –¥–ª—è —Ä–∞–∑–º–∏–Ω–∫–∏
- –î–∞–ª–µ–µ: —Ä–µ–∞–≥–∏—Ä—É–π –Ω–∞ –æ—Ç–≤–µ—Ç—ã, —Ö–≤–∞–ª–∏ –ø—Ä–æ–≥—Ä–µ—Å—Å, –¥–∞–≤–∞–π —Å–ª–µ–¥—É—é—â–µ–µ –∑–∞–¥–∞–Ω–∏–µ –ø–æ –ø–æ—Ä—è–¥–∫—É
- –ï—Å–ª–∏ —É—á–µ–Ω–∏–∫ –Ω–µ –ø–æ–Ω—è–ª: –æ–±—ä—è—Å–Ω–∏ –ø—Ä–æ—â–µ, –ø—Ä–∏–≤–µ–¥–∏ –ø—Ä–∏–º–µ—Ä –∏–∑ –∂–∏–∑–Ω–∏

–ò—Å—Ç–æ—Ä–∏—è —Ä–∞–∑–≥–æ–≤–æ—Ä–∞:
${messages.map(m => `${m.role === 'user' ? '–£—á–µ–Ω–∏–∫' : '–Æ–ª–∏—è'}: ${m.content}`).join('\n')}

–£—á–µ–Ω–∏–∫: ${userMessage}

–û—Ç–≤–µ—Ç—å –∫–∞–∫ –Æ–ª–∏—è (–∫—Ä–∞—Ç–∫–æ, –æ–¥–Ω–æ –∑–∞–¥–∞–Ω–∏–µ, –Ω–∞ –ø–∞–ª—å—Ü–∞—Ö).

–í –ö–û–ù–¶–ï –û–¢–í–ï–¢–ê –¥–æ–±–∞–≤—å —Ä–∞–∑–¥–µ–ª "–ö–ª—é—á–µ–≤—ã–µ —Ç–µ–∑–∏—Å—ã" —Å 2-3 –∫–æ—Ä–æ—Ç–∫–∏–º–∏ —Ç–µ–∑–∏—Å–∞–º–∏ (–∫–∞–∂–¥—ã–π –Ω–µ –±–æ–ª–µ–µ 15 —Å–ª–æ–≤), –∫–æ—Ç–æ—Ä—ã–µ –ø–æ–º–æ–≥—É—Ç —É—á–µ–Ω–∏–∫—É –±—ã—Å—Ç—Ä–æ —É–ª–æ–≤–∏—Ç—å —Å—É—Ç—å. –§–æ—Ä–º–∞—Ç:
–ö–ª—é—á–µ–≤—ã–µ —Ç–µ–∑–∏—Å—ã
1. –ü–µ—Ä–≤—ã–π —Ç–µ–∑–∏—Å
2. –í—Ç–æ—Ä–æ–π —Ç–µ–∑–∏—Å
3. –¢—Ä–µ—Ç–∏–π —Ç–µ–∑–∏—Å`;

    console.log('üì§ Sending to LLM with lesson context:', lessonContext ? 'YES' : 'NO');
    console.log('üåç Julia always speaks Russian');
    if (lessonContext) {
      console.log('üìñ Lesson:', lessonContext.title, '|', lessonContext.topic);
    }

    const response = await fetch('/api/responses', {
        method: 'POST',
      headers: { 'Content-Type': 'application/json' },
        body: JSON.stringify({
        model: 'gpt-5.1',
        input: systemPrompt
        })
      });

      if (!response.ok) {
      throw new Error('LLM failed');
    }

    const result = await response.json();
    return result.output_text;
  };

  // Speak text
  const speakText = async (text: string): Promise<void> => {
    try {
      console.log('üîä Speaking:', text.substring(0, 30) + '...');

      const response = await fetch('/api/audio/speech', {
        method: 'POST',
        headers: { 'Content-Type': 'application/json' },
        body: JSON.stringify({
          model: 'tts-1',
          input: text,
          voice: 'nova'
        })
      });

      if (!response.ok) {
        throw new Error('TTS failed');
      }

      const audioBlob = await response.blob();
      const audioUrl = URL.createObjectURL(audioBlob);

      return new Promise((resolve, reject) => {
      const audio = new Audio(audioUrl);

      audio.onended = () => {
        URL.revokeObjectURL(audioUrl);
          console.log('‚úÖ TTS complete');
          resolve();
      };

      audio.onerror = (error) => {
          console.error('‚ùå TTS playback error:', error);
        URL.revokeObjectURL(audioUrl);
          reject(error);
      };

        audio.play().catch((playError) => {
          // Handle autoplay restrictions
          if (playError.name === 'NotAllowedError') {
            console.warn('‚ö†Ô∏è Autoplay blocked by browser. User interaction required.');
            URL.revokeObjectURL(audioUrl);
            resolve(); // Continue without sound
          } else {
            reject(playError);
          }
        });
      });

    } catch (error) {
      console.error('‚ùå TTS error, using fallback:', error);

      // Fallback to browser TTS
      return new Promise((resolve) => {
        const utterance = new SpeechSynthesisUtterance(text);
        utterance.lang = 'ru-RU';
        utterance.onend = () => {
          console.log('‚úÖ Browser TTS complete');
          resolve();
        };
        utterance.onerror = () => resolve();
        window.speechSynthesis.speak(utterance);
      });
    }
  };

  // Load lesson context from localStorage - ONCE at mount
  useEffect(() => {
    // Skip if already loaded
    if (lessonContextRef.current) {
      console.log('‚úÖ Lesson context already loaded, skipping');
      return;
    }

    try {
      const storedLesson = localStorage.getItem('currentLesson');
      console.log('üîç Checking localStorage for currentLesson...');
      
      if (storedLesson) {
        const lessonData = JSON.parse(storedLesson);
        const context = {
          title: lessonData.title || '–£—Ä–æ–∫',
          topic: lessonData.topic || '',
          description: lessonData.description || lessonData.aspects || lessonData.content || ''
        };
        
        lessonContextRef.current = context;
        console.log('üìö Lesson context loaded ONCE:');
        console.log('  Title:', context.title);
        console.log('  Topic:', context.topic);
        console.log('  Description:', context.description?.substring(0, 100) + '...');
      } else {
        console.warn('‚ö†Ô∏è No lesson context found in localStorage');
      }
    } catch (error) {
      console.error('‚ùå Error loading lesson context:', error);
    }
  }, []);

  // Mount effect
  useEffect(() => {
    console.log('üéì VoiceCallPage mounted');
    startListening();

    return () => {
      console.log('üéì VoiceCallPage unmounting');
      cleanup();
    };
  }, []);

  // Video control effect
  useEffect(() => {
    if (videoRef.current) {
      if (isSpeaking) {
        // TTS speaking - loop video
        videoRef.current.loop = true;
        videoRef.current.play().catch(console.error);
    } else {
        // Not speaking - pause at 00:00
        videoRef.current.loop = false;
        videoRef.current.pause();
        videoRef.current.currentTime = 0;
      }
    }
  }, [isSpeaking]);

  return (
    <div className="min-h-screen bg-gray-50">
      <Header />
      <div className="container mx-auto px-4 py-8">
        <div className="max-w-4xl mx-auto">
          <Button
            variant="ghost"
            onClick={() => navigate(-1)}
            className="mb-4 flex items-center gap-2"
          >
            <ArrowLeft className="w-4 h-4" />
            –í–µ—Ä–Ω—É—Ç—å—Å—è –Ω–∞–∑–∞–¥
          </Button>

          <Card>
            <CardHeader>
              <CardTitle className="flex items-center gap-2">
                <Mic className="w-5 h-5" />
              </CardTitle>
            </CardHeader>
            <CardContent className="space-y-6">
              {/* Video Avatar */}
              <div className="text-center">
                <div className="relative inline-block">
                  <video
                    ref={videoRef}
                    className="w-48 h-48 rounded-full object-cover border-4 border-gray-200 shadow-lg"
                    muted
                    playsInline
                  >
                    <source src="/Untitled Video.mp4" type="video/mp4" />
                  </video>

                  {/* Status overlay */}
                  <div className="absolute -bottom-2 left-1/2 transform -translate-x-1/2 bg-white px-3 py-1 rounded-full shadow-md border">
                    {isListening && (
                      <div className="flex items-center gap-2 text-green-600">
                        <div className="w-2 h-2 bg-green-600 rounded-full animate-pulse"></div>
                        <span className="text-sm font-medium">–°–ª—É—à–∞–µ—Ç</span>
                  </div>
                )}
                {isProcessing && (
                      <div className="flex items-center gap-2 text-blue-600">
                        <Loader2 className="w-3 h-3 animate-spin" />
                        <span className="text-sm font-medium">–û–±—Ä–∞–±–∞—Ç—ã–≤–∞–µ—Ç</span>
                  </div>
                )}
                {isSpeaking && (
                      <div className="flex items-center gap-2 text-purple-600">
                        <div className="w-2 h-2 bg-purple-600 rounded-full animate-pulse"></div>
                        <span className="text-sm font-medium">–ì–æ–≤–æ—Ä–∏—Ç</span>
                      </div>
                    )}
                  </div>
                </div>
              </div>


              {/* Control buttons */}
              <div className="flex justify-center gap-4">
                <Button
                  variant={isMuted ? "destructive" : "outline"}
                  onClick={toggleMute}
                  className="flex items-center gap-2"
                >
                  {isMuted ? <MicOff className="w-4 h-4" /> : <Mic className="w-4 h-4" />}
                  {isMuted ? '–í–∫–ª—é—á–∏—Ç—å –º–∏–∫—Ä–æ—Ñ–æ–Ω' : '–í—ã–∫–ª—é—á–∏—Ç—å –º–∏–∫—Ä–æ—Ñ–æ–Ω'}
                </Button>

                <Button
                  variant="outline"
                  onClick={endLesson}
                  className="flex items-center gap-2 text-red-600 hover:text-red-700 hover:bg-red-50"
                >
                  <PhoneOff className="w-4 h-4" />
                  –ó–∞–≤–µ—Ä—à–∏—Ç—å —É—Ä–æ–∫
                </Button>
              </div>

              {/* Key Theses */}
              {speechTheses.length > 0 && (
                <div className="bg-gradient-to-r from-blue-50 to-indigo-50 border border-blue-200 rounded-lg p-4 shadow-sm mt-4">
                  <h3 className="text-sm font-semibold text-blue-900 mb-3 flex items-center gap-2">
                    <span className="w-2 h-2 bg-blue-500 rounded-full"></span>
                    –ö–ª—é—á–µ–≤—ã–µ —Ç–µ–∑–∏—Å—ã
                  </h3>
                  <ol className="space-y-2">
                    {speechTheses.map((thesis, index) => (
                      <li key={index} className="text-sm text-gray-700 flex items-start gap-2">
                        <span className="flex-shrink-0 w-5 h-5 rounded-full bg-blue-500 text-white text-xs flex items-center justify-center font-medium">
                          {index + 1}
                        </span>
                        <span className="flex-1">{thesis}</span>
                      </li>
                    ))}
                  </ol>
                  <button
                    onClick={() => setSpeechTheses([])}
                    className="mt-3 text-xs text-blue-600 hover:text-blue-800 underline"
                  >
                    –°–∫—Ä—ã—Ç—å
                  </button>
                </div>
              )}

              {/* Error */}
              {error && (
                <div className="p-4 bg-red-50 border border-red-200 rounded-lg text-red-700">
                  {error}
                </div>
              )}

            </CardContent>
          </Card>
        </div>
      </div>
    </div>
  );
};

export default VoiceCallPage;
